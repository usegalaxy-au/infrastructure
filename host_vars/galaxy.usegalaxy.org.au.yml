# Specific settings for galaxy dev application/web server

galaxy_config_file: /opt/galaxy/galaxy.yml

# variables for attaching mounted volume to application server
attached_volumes: # TODO: check this
  - device: /dev/vdb
    path: /mnt
    fstype: ext4
    partition: 1

certbot_domains:
  - "{{ hostname }}" # galaxy.usegalaxy.org.au
  - "usegalaxy.org.au"
  - "www.usegalaxy.org.au"
  - "*.interactivetoolentrypoint.interactivetool.usegalaxy.org.au"
  - "*.ep.interactivetool.usegalaxy.org.au"
  - "genome.usegalaxy.org.au"
  - "proteomics.usegalaxy.org.au"
certbot_dns_provider: cloudflare
certbot_dns_credentials:
  api_token: "{{ vault_dns_cloudflare_api_token }}"
certbot_dns_provider_propagation_seconds: 60

nginx_ssl_servers:
  - galaxy
  - galaxy-gie-proxy ## we still do this?

# gie proxy hostname
interactive_tools_server_name: "usegalaxy.org.au"

# galaxy_repo: https://github.com/galaxyproject/galaxy.git
# galaxy_commit_id: release_24.0
galaxy_repo: https://github.com/usegalaxy-au/galaxy.git
galaxy_commit_id: release_23.1_au

# TODO: check list concatenation with versions of ansible > 2.12
shared_mounts: "{{ galaxy_server_and_worker_shared_mounts + galaxy_web_server_mounts }}" # sourced from galaxy_etca.yml
galaxy_user_data_nfs_opts: 'noatime,actimeo=1,defaults' # galaxy VM needs to be able to see files as soon as they arrive in user data folders

# ansible-galaxy
galaxy_dynamic_job_rules_src_dir: files/galaxy/dynamic_job_rules/production
galaxy_dynamic_job_rules_dir: "{{ galaxy_root }}/dynamic_job_rules"
galaxy_dynamic_job_rules:
  - total_perspective_vortex/tools.yml
  - total_perspective_vortex/destinations.yml.j2
  - total_perspective_vortex/users.yml
  - total_perspective_vortex/roles.yml
  - total_perspective_vortex/default_tool.yml.j2
  - readme.txt

galaxy_systemd_mode: gravity
galaxy_systemd_env:
  [DRMAA_LIBRARY_PATH="/usr/lib/slurm-drmaa/lib/libdrmaa.so.1"]

galaxy_tools_indices_dir: /mnt/tools
galaxy_custom_indices_dir: /mnt/custom-indices
galaxy_tmp_dir: /mnt/tmp
galaxy_tus_upload_store: "{{ galaxy_tmp_dir }}/tus"

galaxy_infrastructure_url: https://usegalaxy.org.au

upload_store_volume_dir: /mnt/user-data-volA
nginx_upload_store_base_dir: "{{ upload_store_volume_dir }}/upload_store"
nginx_upload_store_dir: "{{ nginx_upload_store_base_dir }}/uploads"
nginx_upload_job_files_store_dir: "{{ nginx_upload_store_base_dir }}/job_files"
nginx_upload_store_set_cleanup_cron_job: true

# galaxy_virtualenv_python: python3.11 # deferred until release 23.2 probably

host_galaxy_config_files:
  - src: "{{ galaxy_config_file_src_dir }}/config/trs_servers_conf.yml"
    dest: "{{ galaxy_config_dir }}/trs_servers_conf.yml"
  - src: "{{ galaxy_config_file_src_dir }}/config/tool_data_table_conf.xml" # TODO: etca will need this too
    dest: "{{ galaxy_config_dir }}/tool_data_table_conf.xml"
  - src: "{{ galaxy_config_file_src_dir }}/config/activation-email.html"
    dest: "{{ galaxy_config_dir }}/mail/activation-email.html"
  - src: "{{ galaxy_config_file_src_dir }}/config/activation-email.txt"
    dest: "{{ galaxy_config_dir }}/mail/activation-email.txt"
  - src: "{{ galaxy_config_file_src_dir }}/config/themes/themes_main.yml"
    dest: "{{ galaxy_config_dir }}/themes_main.yml"
  - src: "{{ galaxy_config_file_src_dir }}/config/themes/themes_genome.yml"
    dest: "{{ galaxy_config_dir }}/themes_genome.yml"
  - src: "{{ galaxy_config_file_src_dir }}/config/themes/themes_proteomics.yml"
    dest: "{{ galaxy_config_dir }}/themes_proteomics.yml"

host_galaxy_config_templates:
  - src: "{{ galaxy_config_template_src_dir }}/config/galaxy_object_store_conf.xml.j2"
    dest: "{{ galaxy_config_dir }}/object_store_conf.xml"
  - src: "{{ galaxy_config_template_src_dir }}/config/galaxy_job_conf.yml.j2"
    dest: "{{ galaxy_config_dir }}/job_conf.yml"
  - src: "{{ galaxy_config_template_src_dir }}/toolbox/filters/ga_filters.py.j2" ## File cannot be created at dest on first run of playbook
    dest: "{{ galaxy_toolbox_filters_dir }}/ga_filters.py"


host_galaxy_config_gravity:
  process_manager: systemd
  galaxy_root: "{{ galaxy_server_dir }}"
  galaxy_user: "{{ galaxy_user.name }}"
  app_server: gunicorn
  virtualenv: "{{ galaxy_venv_dir }}"
  celery:
    enable: false
    enable_beat: false
  gunicorn:
    - bind: unix:{{ galaxy_mutable_config_dir }}/gunicorn1.sock
      workers: 2
      # Other options that will be passed to gunicorn
      extra_args: '--forwarded-allow-ips="*"'
      timeout: 1800
      restart_timeout: 1800
      preload: true
      environment: "{{ galaxy_process_env }}"
    - bind: unix:{{ galaxy_mutable_config_dir }}/gunicorn2.sock
      workers: 2
      # Other options that will be passed to gunicorn
      extra_args: '--forwarded-allow-ips="*"'
      timeout: 1800
      restart_timeout: 1800
      preload: true
      environment: "{{ galaxy_process_env }}"
  tusd:
    enable: true
    tusd_path: /usr/local/sbin/tusd
    upload_dir: "{{ galaxy_tus_upload_store }}"
  gx_it_proxy:
    enable: true
    version: '>=0.0.5'  # default from galaxy.yml.sample
    ip: "{{ gie_proxy_ip }}"
    port: "{{ gie_proxy_port }}"
    sessions: "{{ gie_proxy_sessions_path }}"
    verbose: true
  reports:
    enable: true
    url_prefix: /reports
    bind: "unix:{{ galaxy_mutable_config_dir }}/reports.sock"
    config_file: "{{ galaxy_config_dir }}/reports.yml"
  ##& handlers and celery are on galaxy-handlers VM with 2-VM galaxy server. Uncomment these entries when using a single VM
  # celery:
  #   concurrency: 2
  #   loglevel: DEBUG
  # handlers:
  #   handler:
  #     environment: "{{ galaxy_process_env }}"
  #     processes: 5
  #     pools:
  #       - job-handlers
  #       - workflow-schedulers

host_galaxy_config: # renamed from __galaxy_config
  gravity: "{{ host_galaxy_config_gravity }}"

  galaxy:
    ##& amqp_internal_connection for 2-VM galaxy server. Comment this out when out when using a single VM
    amqp_internal_connection: "pyamqp://galaxy_queues:{{ vault_rabbitmq_password_galaxy_prod }}@{{ hostvars['galaxy-queue']['internal_ip'] }}:5671//galaxy/galaxy_queues?ssl=1"
    admin_users: "{{ machine_users | selectattr('email', 'defined') | selectattr('roles', 'contains', 'galaxy_admin') | map(attribute='email') | join(',') }},{{ bpa_email }}"
    brand: "Australia"
    database_connection: "postgresql://galaxy:{{ galaxy_db_user_password }}@{{ hostvars['galaxy-db']['internal_ip'] }}:5432/galaxy"
    id_secret: "{{ vault_aarnet_id_secret }}" # TODO: this need to stay the same wherever production galaxy is running, but the name of the vault variable is misleading
    object_store_config_file: "{{ galaxy_config_dir }}/object_store_conf.xml"
    smtp_server: localhost
    ga_code: "{{ vault_prod_ga_code }}" ## swich off ga_code while testing.  TODO: uncomment this when we go live
    interactivetools_enable: true
    interactivetools_map: "{{ gie_proxy_sessions_path }}"
    enable_oidc: true
    oidc_config_file: "{{ galaxy_config_dir }}/oidc_config.xml"
    oidc_backends_config_file: "{{ galaxy_config_dir }}/oidc_backends_config.xml"
    tool_data_table_config_path: "{{ galaxy_config_dir }}/tool_data_table_conf.xml,{{ galaxy_mutable_config_dir }}/shed_tool_data_table_conf.xml,/cvmfs/data.galaxyproject.org/byhand/location/tool_data_table_conf.xml,/cvmfs/data.galaxyproject.org/managed/location/tool_data_table_conf.xml"
    # nginx upload module
    nginx_upload_store: "{{ nginx_upload_store_dir }}"
    nginx_upload_path: "/_upload"
    nginx_upload_job_files_store: "{{ nginx_upload_job_files_store_dir }}"
    nginx_upload_job_files_path: "/_job_files"
    job_config_file: "{{ galaxy_config_dir }}/job_conf.yml"
    watch_job_rules: true # important for total perspective vortex
    enable_mulled_containers: true
    enable_beta_containers_interface: true # TODO: is one of this or the above config options deprecated?
    tool_filters: ga_filters:hide_test_tools,ga_filters:restrict_alphafold
    #TRS - workflowhub
    trs_servers_config_file: "{{ galaxy_config_dir }}/trs_servers_conf.yml"
    # TUS
    tus_upload_store: "{{ galaxy_tus_upload_store }}"

    # Allow cross-subdomain cookie sharing:
    cookie_domain: usegalaxy.org.au

    user_activation_on: true
    activation_email: <activation-noreply@usegalaxy.org.au>

    sentry_dsn: "{{ vault_sentry_url_galaxy_production }}"

    celery_conf:
      result_backend: "redis://:{{ vault_redis_requirepass }}@{{ hostvars['galaxy-queue']['internal_ip'] }}:6379/0"
      # result_backend: "{{ redis_connection_string }}"
      task_routes:
        galaxy.fetch_data: disabled
        # galaxy.fetch_data: galaxy.external
        galaxy.set_job_metadata: galaxy.external

    # Offload long-running tasks to a Celery task queue. Activate this
    # only if you have setup a Celery worker for Galaxy. For details, see
    # https://docs.galaxyproject.org/en/master/admin/production.html
    enable_celery_tasks: true

    themes_config_file_by_host:
      usegalaxy.org.au: "{{ galaxy_config_dir }}/themes_main.yml"
      genome.usegalaxy.org.au: "{{ galaxy_config_dir }}/themes_genome.yml"
      proteomics.usegalaxy.org.au: "{{ galaxy_config_dir }}/themes_proteomics.yml"

# cvmfs
cvmfs_cache_base: /mnt/var/lib/cvmfs

# vars for setting up .pgpass # TODO: can this be generalised for group_vars/galaxyservers.yml instead of individual host files?
pg_db_password:
  galaxy: "{{ galaxy_db_user_password }}"
  reader: "{{ galaxy_db_reader_password }}"
  tiaasadmin: "{{ galaxy_db_tiaasadmin_password }}"
db_address: "{{ hostvars['galaxy-db']['internal_ip'] }}"
gxadmin_ubuntu_config_dir: /home/ubuntu/.config # TODO: is this variable really needed when it is exactly like other user paths for gxadmin config?

# vars for stats_collection
stats_dir: /home/ubuntu/stats_collection
stats_instance: main
sinfo_hostname: 'Galaxy-Main'
stats_db_server: "{{ hostvars['galaxy-db']['internal_ip'] }}"
stats_db_user: reader
influx_url: "stats.usegalaxy.org.au"
influx_db_stats: "GA_server"
stats_db_password: "{{ galaxy_db_reader_password }}"
influx_salt: "{{ prod_queue_size_salt }}"
add_daily_stats: true
add_monthly_stats: true
add_utilisation_info: true
add_queue_info: true
add_volume_usage_info: true
stats_galaxy_volume_list: "{{ galaxy_server_and_worker_shared_mounts | selectattr('state', 'equalto', 'mounted') | map(attribute='path') | list }}"
stats_galaxy_volume_db: "GA_server"

# host-specific settings for postfix
postfix_host_domain: "usegalaxy.org.au"
postfix_hostname: "galaxy"
smtp_login: "{{ vault_smtp_login_prod }}"
smtp_password: "{{ vault_smtp_password_prod }}"

#Vars for TIaaS
tiaas_galaxy_db_host: "{{ hostvars['galaxy-db']['internal_ip'] }}"
tiaas_galaxy_db_port: "5432"
tiaas_galaxy_db_user: "tiaas"
tiaas_galaxy_db_pass: "{{ galaxy_db_tiaas_password }}"
tiaas_info:
  owner: "Galaxy Australia"
  owner_email: help@genome.edu.au
  owner_site: "https://site.usegalaxy.org.au"
  domain: usegalaxy.org.au
tiaas_other_config: |
  EMAIL_HOST="localhost"
  EMAIL_BACKEND = 'django.core.mail.backends.smtp.EmailBackend'
  EMAIL_TIMEOUT = 60
  TIAAS_SEND_EMAIL_TO = "help@genome.edu.au"
  TIAAS_SEND_EMAIL_FROM = "tiaas-no-reply@usegalaxy.org.au"
  TIAAS_SEND_EMAIL_TO_REQUESTER = True
  TIAAS_LATE_REQUEST_PREVENTION_DAYS = 0
  TIAAS_GDPR_AUTO_REDACT = False

# Create a cron job to disassociate training roles from groups after trainings have expired, set to `false` to disable
tiaas_disassociate_training_roles:
  hour: 9 # optional, defaults to 0
  minute: 0 # optional, defaults to 0

tiaas_show_advertising: false
tiaas_retain_contact_consent: false

# Templates to override web content:
tiaas_templates_dir: files/tiaas/html
# Static files referenced by above templates:
tiaas_extra_static_dir: files/tiaas/static

# AAF specific settings
aaf_issuer_url: "{{ vault_aaf_issuer_url_prod }}"
aaf_client_id: "{{ vault_aaf_client_id_prod }}"
aaf_client_secret: "{{ vault_aaf_client_secret_prod }}"

# remote-pulsar-cron variables
rpc_skip_cron_setup: false
rpc_db_connection_string: "postgres://reader:{{ galaxy_db_reader_password }}@{{ hostvars['galaxy-db']['internal_ip'] }}:5432/galaxy"

rpc_pulsar_machines:
  - pulsar_name: pulsar-mel3
    pulsar_ip_address: "{{ hostvars['pulsar-mel3']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "17"
    cron_minute: "00"
  - pulsar_name: pulsar-mel2
    pulsar_ip_address: "{{ hostvars['pulsar-mel2']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "5,11,17,23"
    cron_minute: "10"
  - pulsar_name: pulsar-paw
    pulsar_ip_address: "{{ hostvars['pulsar-paw']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "17"
    cron_minute: "20"
  - pulsar_name: pulsar-high-mem1
    pulsar_ip_address: "{{ hostvars['pulsar-high-mem1']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "17"
    cron_minute: "30"
  - pulsar_name: pulsar-high-mem2
    pulsar_ip_address: "{{ hostvars['pulsar-high-mem2']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "17"
    cron_minute: "40"
  - pulsar_name: qld-pulsar-himem-0
    pulsar_ip_address: "{{ hostvars['qld-pulsar-himem-0']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "17"
    cron_minute: "50"
  - pulsar_name: qld-pulsar-himem-1
    pulsar_ip_address: "{{ hostvars['qld-pulsar-himem-1']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "18"
    cron_minute: "00"
  - pulsar_name: qld-pulsar-himem-2
    pulsar_ip_address: "{{ hostvars['qld-pulsar-himem-2']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "18"
    cron_minute: "10"
  - pulsar_name: pulsar-nci-training
    pulsar_ip_address: "{{ hostvars['pulsar-nci-training']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "18"
    cron_minute: "20"
  - pulsar_name: pulsar-qld-blast
    pulsar_ip_address: "{{ hostvars['pulsar-qld-blast']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "18"
    cron_minute: "30"
  - pulsar_name: pulsar-QLD
    pulsar_ip_address: "{{ hostvars['pulsar-QLD']['ansible_ssh_host'] }}"
    pulsar_staging_dir: /mnt/tmp/files/staging
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "18"
    cron_minute: "40"
  - pulsar_name: pulsar-azure-0
    pulsar_ip_address: "{{ hostvars['pulsar-azure-0']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "18"
    cron_minute: "50"
    remote_user: hpcuser

extra_keys:
  - id: ubuntu_maintenance_key
    type: private
  - id: internal_hop_key
    type: private
  - id: internal_hop_key
    type: public
    from: "{{ hostvars['galaxy-backup']['internal_ip'] }},{{ hostvars['galaxy-handlers']['internal_ip'] }},{{ hostvars['galaxy-queue']['internal_ip'] }}"

# # grt-sender role # TODO: what of GRT??
# grt_sender_dir: /mnt/var/galactic_radio_telescope
# grt_sender_api_key: "{{ vault_grt_api_key }}"
# grt_sender_grt_url: https://telescope.usegalaxy.org.au/grt

# Docker
docker_users:
  - "{{ galaxy_user.name }}"
docker_daemon_options:
  data-root: /mnt/docker-data

# Job conf limits (defined here to be available to job conf and tpv)
job_conf_limits:
  environments:
    interactive_pulsar:
      tags:
      - registered_user_concurrent_jobs_10
    pulsar-QLD:
      tags:
      - registered_user_concurrent_jobs_10
    pulsar-azure:
      tags:
      - registered_user_concurrent_jobs_10
    pulsar-azure-1-gpu:
      tags:
      - registered_user_concurrent_jobs_25
    pulsar-azure-gpu:
      tags:
      - registered_user_concurrent_jobs_10
    pulsar-high-mem1:
      tags:
      - registered_user_concurrent_jobs_10
    pulsar-high-mem2:
      tags:
      - registered_user_concurrent_jobs_10
    pulsar-mel2:
      tags:
      - registered_user_concurrent_jobs_10
    pulsar-mel3:
      tags:
      - registered_user_concurrent_jobs_10
    pulsar-nci-training:
      tags:
      - registered_user_concurrent_jobs_10
    pulsar-paw:
      tags:
      - registered_user_concurrent_jobs_10
    pulsar-qld-blast:
      tags:
      - registered_user_concurrent_jobs_10
    pulsar-qld-high-mem0:
      tags:
      - registered_user_concurrent_jobs_10
    pulsar-qld-high-mem1:
      tags:
      - registered_user_concurrent_jobs_10
    pulsar-qld-high-mem2:
      tags:
      - registered_user_concurrent_jobs_10
    slurm:
      tags:
      - registered_user_concurrent_jobs_10
    slurm-training:
      tags:
      - registered_user_concurrent_jobs_10
    pulsar-qld-gpu1:
      tags:
      - registered_user_concurrent_jobs_10
  limits:
  - type: anonymous_user_concurrent_jobs
    value: 1
  - type: destination_user_concurrent_jobs
    tag: registered_user_concurrent_jobs_10
    value: 10
  - type: destination_user_concurrent_jobs
    tag: registered_user_concurrent_jobs_25
    value: 25

  - type: destination_total_concurrent_jobs
    id: slurm
    value: 80
  - type: destination_user_concurrent_jobs
    id: slurm
    value: 5

  - type: destination_total_concurrent_jobs
    id: slurm-training
    value: 40
  - type: destination_user_concurrent_jobs
    id: slurm-training
    value: 4

  - type: destination_total_concurrent_jobs
    id: pulsar-mel2
    value: 20
  - type: destination_user_concurrent_jobs
    id: pulsar-mel2
    value: 5

  - type: destination_total_concurrent_jobs
    id: pulsar-mel3
    value: 20
  - type: destination_user_concurrent_jobs
    id: pulsar-mel3
    value: 5

  - type: destination_total_concurrent_jobs
    id: pulsar-paw
    value: 10

  - type: destination_user_concurrent_jobs
    id: pulsar-nci-training
    value: 3

  - type: destination_total_concurrent_jobs
    id: pulsar-high-mem1
    value: 6

  - type: destination_total_concurrent_jobs
    id: pulsar-high-mem2
    value: 6

  - type: destination_total_concurrent_jobs
    id: pulsar-qld-high-mem0
    value: 20

  - type: destination_total_concurrent_jobs
    id: pulsar-qld-high-mem1
    value: 20

  - type: destination_total_concurrent_jobs
    id: pulsar-qld-high-mem2
    value: 20

  - type: destination_total_concurrent_jobs
    id: pulsar-QLD
    value: 20
  - type: destination_user_concurrent_jobs
    id: pulsar-QLD
    value: 5

  - type: destination_total_concurrent_jobs
    id: pulsar-qld-blast
    value: 6
  - type: destination_user_concurrent_jobs
    id: pulsar-qld-blast
    value: 2

  - type: destination_total_concurrent_jobs
    id: pulsar-azure
    value: 4
  - type: destination_user_concurrent_jobs
    id: pulsar-azure
    value: 1

  - type: destination_total_concurrent_jobs
    id: pulsar-azure-gpu
    value: 8
  - type: destination_user_concurrent_jobs
    id: pulsar-azure-gpu
    value: 2

# Singularity and docker volumes
slurm_singularity_volumes_list:
  - $job_directory:rw
  - $galaxy_root:ro
  - $tool_directory:ro
  - /mnt/user-data-volA:ro
  - /mnt/user-data-volB:ro
  - /mnt/user-data-volC:ro
  - "{{ qld_file_mounts_path }}:ro"
  - "{{ pawsey_file_mounts_path }}:ro"
  - /mnt/custom-indices:ro
  - /cvmfs/data.galaxyproject.org:ro
  - /tmp:rw

pulsar_singularity_volumes_list:
  - $job_directory:rw
  - $tool_directory:ro
  - /mnt/custom-indices:ro
  - /cvmfs/data.galaxyproject.org:ro
  - /tmp:rw

slurm_docker_volumes_list: "{{ slurm_singularity_volumes_list }}"
pulsar_docker_volumes_list: "{{ pulsar_singularity_volumes_list }}"

# comma separated strings for the job conf
slurm_singularity_volumes: "{{ slurm_singularity_volumes_list | join(',') }}"
pulsar_singularity_volumes: "{{ pulsar_singularity_volumes_list | join(',') }}"
slurm_docker_volumes: "{{ slurm_docker_volumes_list | join(',') }}"
pulsar_docker_volumes: "{{ pulsar_docker_volumes_list | join(',') }}"

singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.8.3"

# delete-tmp-jwds role for removing job working directories from /mnt/tmp
dt_remote_ip: "{{ hostvars['galaxy-job-nfs']['internal_ip'] }}"
dt_ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
dt_cron_hour: "20"
dt_cron_minute: "00"
dt_skip_cron_setup: true
dt_connection_string: "postgres://reader:{{ galaxy_db_reader_password }}@{{ hostvars['galaxy-db']['internal_ip'] }}:5432/galaxy" # TODO: generalise connection string

# NFS stuff (for exporting so galaxy-handlers can mount)
nfs_exports:
  - "{{ galaxy_root }}  *(rw,async,no_root_squash,no_subtree_check)"

# Subdomains
galaxy_subsite_base_domain: "usegalaxy.org.au"
galaxy_subsite_default_welcome: "https://site.usegalaxy.org.au"
galaxy_subsite_dir: /mnt/galaxy/subsites
galaxy_subsites:
  - name: genome
    brand: Genome Lab
    iframe: "https://site.usegalaxy.org.au/landing/genome"
    wallpaper: false # If wallpaper is true, then files/subsites/{name}.png will be copied to /static/dist/{name}.png, and can be used exactly like that in the CSS.
    tool_sections: []
    custom_css: |
      #masthead .navbar-nav>li.active {
        background: #2c3067 !important;
      }
      #masthead {
        background: #3a3e87 !important;
      }
      #masthead a.navbar-brand::after {
        content: "Australia - Genome Lab";
      }
      #masthead .navbar-text {
        display: none;
      }
  - name: proteomics
    hidden: true
    brand: Proteomics lab
    iframe: "https://site.usegalaxy.org.au/landing/proteomics"
    wallpaper: false # If wallpaper is true, then files/subsites/{name}.png will be copied to /static/dist/{name}.png, and can be used exactly like that in the CSS.
    tool_sections: []
    custom_css: |
      #masthead .navbar-nav>li.active {
        background: #402668 !important;
      }
      #masthead {
        background: #542a95 !important;
      }
      #masthead a.navbar-brand::after {
        content: "Australia - Proteomics Lab";
      }
      #masthead .navbar-text {
        display: none;
      }

webhook_plugins:
  - subdomain_switcher
  # These ones aren't in the playbook as they are included in the galaxy repo:
  - demo
  - gtn
  - news

# ssh config, only for ubuntu
ssh_config_id_file: "/home/{{ ssh_config_user }}/.ssh/internal_hop_key"
ssh_config_user: ubuntu
ssh_config_hosts: "{{ groups['galaxy_group'] }}"
