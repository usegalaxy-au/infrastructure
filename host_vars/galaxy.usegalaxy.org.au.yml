# Specific settings for galaxy dev application/web server

galaxy_config_file: /opt/galaxy/galaxy.yml

# variables for attaching mounted volume to application server
attached_volumes: # TODO: check this
  - device: /dev/vdb
    path: /mnt
    fstype: ext4
    partition: 1

common_logrotate_manage_rsyslog: true

certbot_domains:
  - "{{ hostname }}" # galaxy.usegalaxy.org.au
  - "usegalaxy.org.au"
  - "www.usegalaxy.org.au"
  - "*.interactivetoolentrypoint.interactivetool.usegalaxy.org.au"
  - "*.ep.interactivetool.usegalaxy.org.au"
  - "genome.usegalaxy.org.au"
  - "proteomics.usegalaxy.org.au"
  - "singlecell.usegalaxy.org.au"
  - "microbiology.usegalaxy.org.au"
certbot_dns_provider: cloudflare
certbot_dns_credentials:
  api_token: "{{ vault_dns_cloudflare_api_token }}"
certbot_dns_provider_propagation_seconds: 60

nginx_ssl_servers:
  - galaxy
  - galaxy-gie-proxy ## we still do this?

# gie proxy hostname
interactive_tools_server_name: "usegalaxy.org.au"

galaxy_repo: https://github.com/galaxyproject/galaxy.git
galaxy_commit_id: release_25.0

galaxy_virtualenv_command: "/usr/bin/python3.11 -m venv"

# TODO: check list concatenation with versions of ansible > 2.12
shared_mounts: "{{ galaxy_server_and_worker_shared_mounts + galaxy_web_server_mounts }}" # sourced from galaxy_etca.yml
galaxy_user_data_nfs_opts: 'noatime,actimeo=1,defaults' # galaxy VM needs to be able to see files as soon as they arrive in user data folders

# ansible-galaxy
galaxy_dynamic_job_rules_src_dir: files/galaxy/dynamic_job_rules/production
galaxy_dynamic_job_rules_dir: "{{ galaxy_root }}/dynamic_job_rules"
galaxy_dynamic_job_rules:
  - total_perspective_vortex/tools.yml
  - total_perspective_vortex/destinations.yml.j2
  - total_perspective_vortex/tool_pulsar_scores.yml
  - total_perspective_vortex/users.yml.j2
  - total_perspective_vortex/default_tool.yml.j2
  - readme.txt

galaxy_systemd_mode: gravity
galaxy_systemd_env:
  [DRMAA_LIBRARY_PATH="/usr/lib/slurm-drmaa/lib/libdrmaa.so.1"]

galaxy_tools_indices_dir: /mnt/tools
galaxy_custom_indices_dir: /mnt/custom-indices
galaxy_tmp_dir: /mnt/tmp
galaxy_tus_upload_store: "{{ galaxy_tmp_dir }}/tus"

galaxy_job_working_directory: /mnt/scratch/job_working_directory

galaxy_infrastructure_url: https://usegalaxy.org.au

upload_store_volume_dir: /mnt/user-data-volA
nginx_upload_store_base_dir: "{{ upload_store_volume_dir }}/upload_store"
nginx_upload_store_dir: "{{ nginx_upload_store_base_dir }}/uploads"
nginx_upload_job_files_store_dir: "{{ nginx_upload_store_base_dir }}/job_files"
nginx_upload_store_set_cleanup_cron_job: true

# galaxy_virtualenv_python: python3.11 # deferred until release 23.2 probably

host_galaxy_config_files:
  - src: "{{ galaxy_config_file_src_dir }}/config/trs_servers_conf.yml"
    dest: "{{ galaxy_config_dir }}/trs_servers_conf.yml"
  - src: "{{ galaxy_config_file_src_dir }}/config/tool_data_table_conf.xml" # TODO: etca will need this too
    dest: "{{ galaxy_config_dir }}/tool_data_table_conf.xml"
  - src: "{{ galaxy_config_file_src_dir }}/config/activation-email.html"
    dest: "{{ galaxy_config_dir }}/mail/activation-email.html"
  - src: "{{ galaxy_config_file_src_dir }}/config/activation-email.txt"
    dest: "{{ galaxy_config_dir }}/mail/activation-email.txt"
  - src: "{{ galaxy_config_file_src_dir }}/config/tool_panel_filters/proteomics.yml"
    dest: "{{ galaxy_config_dir }}/tool_panel_filters/proteomics.yml"
  - src: "{{ galaxy_config_file_src_dir }}/config/tool_panel_filters/singlecell.yml"
    dest: "{{ galaxy_config_dir }}/tool_panel_filters/singlecell.yml"
  - src: "{{ galaxy_config_file_src_dir }}/config/tool_panel_filters/microbiology.yml"
    dest: "{{ galaxy_config_dir }}/tool_panel_filters/microbiology.yml"
  - src: "{{ galaxy_config_file_src_dir }}/config/error_report.yml"
    dest: "{{ galaxy_config_dir}}/error_report.yml"

host_galaxy_config_templates:
  - src: "{{ galaxy_config_template_src_dir }}/config/galaxy_object_store_conf.yml.j2"
    dest: "{{ galaxy_config_dir }}/object_store_conf.yml"
  - src: "{{ galaxy_config_template_src_dir }}/config/galaxy_job_conf.yml.j2"
    dest: "{{ galaxy_config_dir }}/job_conf.yml"
  - src: "{{ galaxy_config_template_src_dir }}/toolbox/filters/ga_filters.py.j2" ## File cannot be created at dest on first run of playbook
    dest: "{{ galaxy_toolbox_filters_dir }}/ga_filters.py"


host_galaxy_config_gravity:
  process_manager: systemd
  galaxy_root: "{{ galaxy_server_dir }}"
  galaxy_user: "{{ galaxy_user.name }}"
  app_server: gunicorn
  virtualenv: "{{ galaxy_venv_dir }}"
  celery:
    enable: false
    enable_beat: false
  gunicorn:
    - bind: "0.0.0.0:8888"
      workers: 2
      # Other options that will be passed to gunicorn
      extra_args: '--forwarded-allow-ips="*"'
      timeout: 1800
      restart_timeout: 1800
      preload: true
      environment: "{{ galaxy_process_env }}"
    - bind: "0.0.0.0:8889"
      workers: 2
      # Other options that will be passed to gunicorn
      extra_args: '--forwarded-allow-ips="*"'
      timeout: 1800
      restart_timeout: 1800
      preload: true
      environment: "{{ galaxy_process_env }}"
  tusd:
    enable: true
    tusd_path: /usr/local/sbin/tusd
    upload_dir: "{{ galaxy_tus_upload_store }}"
  gx_it_proxy:
    enable: true
    ip: "{{ gie_proxy_ip }}"
    port: "{{ gie_proxy_port }}"
    sessions: "{{ gie_proxy_sessions_path }}"
    verbose: true
  reports:
    enable: true
    url_prefix: /reports
    bind: "unix:{{ galaxy_mutable_config_dir }}/reports.sock"
    config_file: "{{ galaxy_config_dir }}/reports.yml"
  ##& handlers and celery are on galaxy-handlers VM with 2-VM galaxy server. Uncomment these entries when using a single VM
  # celery:
  #   concurrency: 2
  #   loglevel: DEBUG
  # handlers:
  #   handler:
  #     environment: "{{ galaxy_process_env }}"
  #     processes: 5
  #     pools:
  #       - job-handlers
  #       - workflow-schedulers

# galaxy vault key
galaxy_vault_encryption_key: "{{ vault_galaxy_vault_encryption_key_prod }}"

host_galaxy_config: # renamed from __galaxy_config
  gravity: "{{ host_galaxy_config_gravity }}"

  galaxy:
    ##& amqp_internal_connection for 2-VM galaxy server. Comment this out when out when using a single VM
    amqp_internal_connection: "pyamqp://galaxy_queues:{{ vault_rabbitmq_password_galaxy_prod }}@{{ hostvars['galaxy-queue']['internal_ip'] }}:5671//galaxy/galaxy_queues?ssl=1"
    admin_users: "{{ machine_users | selectattr('email', 'defined') | selectattr('roles', 'contains', 'galaxy_admin') | map(attribute='email') | join(',') }},{{ bpa_email }}"
    brand: "Australia"
    database_connection: "postgresql://galaxy:{{ galaxy_db_user_password }}@{{ hostvars['galaxy-db']['internal_ip'] }}:5432/galaxy"
    id_secret: "{{ vault_aarnet_id_secret }}" # TODO: this need to stay the same wherever production galaxy is running, but the name of the vault variable is misleading
    object_store_config_file: "{{ galaxy_config_dir }}/object_store_conf.yml"
    smtp_server: localhost
    interactivetools_enable: true
    interactivetools_map: "{{ gie_proxy_sessions_path }}"
    enable_oidc: true
    oidc_config_file: "{{ galaxy_config_dir }}/oidc_config.xml"
    oidc_backends_config_file: "{{ galaxy_config_dir }}/oidc_backends_config.xml"
    tool_data_table_config_path: "{{ galaxy_config_dir }}/tool_data_table_conf.xml,{{ galaxy_mutable_config_dir }}/shed_tool_data_table_conf.xml,/cvmfs/data.galaxyproject.org/byhand/location/tool_data_table_conf.xml,/cvmfs/data.galaxyproject.org/managed/location/tool_data_table_conf.xml"
    # nginx upload module
    nginx_upload_store: "{{ nginx_upload_store_dir }}"
    nginx_upload_path: "/_upload"
    nginx_upload_job_files_store: "{{ nginx_upload_job_files_store_dir }}"
    nginx_upload_job_files_path: "/_job_files"
    job_config_file: "{{ galaxy_config_dir }}/job_conf.yml"
    watch_job_rules: true # important for total perspective vortex
    enable_mulled_containers: true
    enable_beta_containers_interface: true # TODO: is one of this or the above config options deprecated?
    tool_filters: ga_filters:hide_test_tools,ga_filters:restrict_alphafold
    #TRS - workflowhub
    trs_servers_config_file: "{{ galaxy_config_dir }}/trs_servers_conf.yml"
    # TUS
    tus_upload_store: "{{ galaxy_tus_upload_store }}"

    # Allow cross-subdomain cookie sharing:
    cookie_domain: usegalaxy.org.au

    user_activation_on: true
    activation_email: <activation-noreply@usegalaxy.org.au>

    # Panel views / themes
    panel_views_dir: "{{ galaxy_config_dir }}/tool_panel_filters"
    default_panel_view_by_host:
      proteomics.usegalaxy.org.au: proteomics
      singlecell.usegalaxy.org.au: singlecell
      microbiology.usegalaxy.org.au: microbiology
    themes_config_file_by_host:
      genome.usegalaxy.org.au: "{{ galaxy_config_dir }}/themes/themes_genome.yml"
      proteomics.usegalaxy.org.au: "{{ galaxy_config_dir }}/themes/themes_proteomics.yml"
      singlecell.usegalaxy.org.au: "{{ galaxy_config_dir }}/themes/themes_singlecell.yml"
      usegalaxy.org.au: "{{ galaxy_config_dir }}/themes/themes_main.yml"

    sentry_dsn: "{{ vault_sentry_url_galaxy_production }}"

    celery_conf:
      result_backend: "redis://:{{ vault_redis_requirepass }}@{{ hostvars['galaxy-queue']['internal_ip'] }}:6379/0"
      # result_backend: "{{ redis_connection_string }}"
      task_routes:
        galaxy.fetch_data: disabled
        # galaxy.fetch_data: galaxy.external
        galaxy.set_job_metadata: galaxy.external

    # Offload long-running tasks to a Celery task queue. Activate this
    # only if you have setup a Celery worker for Galaxy. For details, see
    # https://docs.galaxyproject.org/en/master/admin/production.html
    enable_celery_tasks: true
    error_report_file: "{{ galaxy_config_dir }}/error_report.yml"

# cvmfs
cvmfs_cache_base: /mnt/var/lib/cvmfs

# vars for setting up .pgpass # TODO: can this be generalised for group_vars/galaxyservers.yml instead of individual host files?
pg_db_password:
  galaxy: "{{ galaxy_db_user_password }}"
  reader: "{{ galaxy_db_reader_password }}"
  tiaasadmin: "{{ galaxy_db_tiaasadmin_password }}"
db_address: "{{ hostvars['galaxy-db']['internal_ip'] }}"
gxadmin_ubuntu_config_dir: /home/ubuntu/.config # TODO: is this variable really needed when it is exactly like other user paths for gxadmin config?

# vars for stats_collection
stats_dir: /home/ubuntu/stats_collection
stats_instance: main
sinfo_hostname: 'Galaxy-Main'
stats_db_server: "{{ hostvars['galaxy-db']['internal_ip'] }}"
stats_db_user: reader
influx_url: "stats.usegalaxy.org.au"
influx_db_stats: "GA_server"
stats_db_password: "{{ galaxy_db_reader_password }}"
influx_salt: "{{ prod_queue_size_salt }}"
add_daily_stats: true
add_monthly_stats: true
add_utilisation_info: true
add_queue_info: true
add_volume_usage_info: true
stats_galaxy_volume_list: "{{ galaxy_server_and_worker_shared_mounts | selectattr('state', 'equalto', 'mounted') | map(attribute='path') | list }}"
stats_galaxy_volume_db: "GA_server"

# host-specific settings for postfix
postfix_host_domain: "usegalaxy.org.au"
postfix_hostname: "galaxy"
smtp_login: "{{ vault_smtp_login_prod }}"
smtp_password: "{{ vault_smtp_password_prod }}"

#Vars for TIaaS
tiaas_galaxy_db_host: "{{ hostvars['galaxy-db']['internal_ip'] }}"
tiaas_galaxy_db_port: "5432"
tiaas_galaxy_db_user: "tiaas"
tiaas_galaxy_db_pass: "{{ galaxy_db_tiaas_password }}"
tiaas_info:
  owner: "Galaxy Australia"
  owner_email: help@genome.edu.au
  owner_site: "https://site.usegalaxy.org.au"
  domain: usegalaxy.org.au
tiaas_other_config: |
  EMAIL_HOST="localhost"
  EMAIL_BACKEND = 'django.core.mail.backends.smtp.EmailBackend'
  EMAIL_TIMEOUT = 60
  TIAAS_SEND_EMAIL_TO = "help@genome.edu.au"
  TIAAS_SEND_EMAIL_FROM = "tiaas-no-reply@usegalaxy.org.au"
  TIAAS_SEND_EMAIL_TO_REQUESTER = True
  TIAAS_LATE_REQUEST_PREVENTION_DAYS = 0
  TIAAS_GDPR_AUTO_REDACT = False

# Create a cron job to disassociate training roles from groups after trainings have expired, set to `false` to disable
tiaas_disassociate_training_roles:
  hour: 9 # optional, defaults to 0
  minute: 0 # optional, defaults to 0

tiaas_show_advertising: false
tiaas_retain_contact_consent: false

# Templates to override web content:
tiaas_templates_dir: files/tiaas/html
# Static files referenced by above templates:
tiaas_extra_static_dir: files/tiaas/static

# AAF specific settings
aaf_issuer_url: "{{ vault_aaf_issuer_url_prod }}"
aaf_client_id: "{{ vault_aaf_client_id_prod }}"
aaf_client_secret: "{{ vault_aaf_client_secret_prod }}"

# remote-pulsar-cron variables
rpc_skip_cron_setup: false
rpc_db_connection_string: "postgres://reader:{{ galaxy_db_reader_password }}@{{ hostvars['galaxy-db']['internal_ip'] }}:5432/galaxy"

rpc_pulsar_machines:
  - pulsar_name: pulsar-mel3
    pulsar_ip_address: "{{ hostvars['pulsar-mel3']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "17"
    cron_minute: "00"
  - pulsar_name: pulsar-mel2
    pulsar_ip_address: "{{ hostvars['pulsar-mel2']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "5,11,17,23"
    cron_minute: "10"
  - pulsar_name: pulsar-high-mem1
    pulsar_ip_address: "{{ hostvars['pulsar-high-mem1']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "17"
    cron_minute: "30"
  - pulsar_name: pulsar-high-mem2
    pulsar_ip_address: "{{ hostvars['pulsar-high-mem2']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "17"
    cron_minute: "40"
  - pulsar_name: pulsar-qld-high-mem0
    pulsar_ip_address: "{{ hostvars['pulsar-qld-high-mem0']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "17"
    cron_minute: "50"
  - pulsar_name: pulsar-qld-high-mem1
    pulsar_ip_address: "{{ hostvars['pulsar-qld-high-mem1']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "18"
    cron_minute: "00"
  - pulsar_name: pulsar-qld-high-mem2
    pulsar_ip_address: "{{ hostvars['pulsar-qld-high-mem2']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "18"
    cron_minute: "10"
  - pulsar_name: pulsar-nci-training
    pulsar_ip_address: "{{ hostvars['pulsar-nci-training']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "18"
    cron_minute: "20"
  - pulsar_name: pulsar-qld-blast
    pulsar_ip_address: "{{ hostvars['pulsar-qld-blast']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "18"
    cron_minute: "30"
  - pulsar_name: pulsar-QLD
    pulsar_ip_address: "{{ hostvars['pulsar-QLD']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "18"
    cron_minute: "40"
  - pulsar_name: pulsar-azure-0
    pulsar_ip_address: "{{ hostvars['pulsar-azure-0']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "18"
    cron_minute: "50"
    remote_user: hpcuser
  - pulsar_name: pulsar-qld-gpu1
    pulsar_ip_address: "{{ hostvars['pulsar-qld-gpu1']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "19"
    cron_minute: "00"
  - pulsar_name: pulsar-qld-gpu2
    pulsar_ip_address: "{{ hostvars['pulsar-qld-gpu2']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "19"
    cron_minute: "10"
  - pulsar_name: pulsar-qld-gpu3
    pulsar_ip_address: "{{ hostvars['pulsar-qld-gpu3']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "19"
    cron_minute: "20"
  - pulsar_name: pulsar-qld-gpu4
    pulsar_ip_address: "{{ hostvars['pulsar-qld-gpu4']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "19"
    cron_minute: "30"
  - pulsar_name: pulsar-qld-gpu5
    pulsar_ip_address: "{{ hostvars['pulsar-qld-gpu5']['ansible_ssh_host'] }}"
    ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
    delete_jwds: true
    keep_error_days: 7
    cron_hour: "19"
    cron_minute: "40"

extra_keys:
  - id: ubuntu_maintenance_key
    type: private
  - id: internal_hop_key
    type: private
  - id: internal_hop_key
    type: public
    from: "{{ hostvars['galaxy-backup']['internal_ip'] }},{{ hostvars['galaxy-handlers']['internal_ip'] }},{{ hostvars['galaxy-queue']['internal_ip'] }}"

# # grt-sender role # TODO: what of GRT??
# grt_sender_dir: /mnt/var/galactic_radio_telescope
# grt_sender_api_key: "{{ vault_grt_api_key }}"
# grt_sender_grt_url: https://telescope.usegalaxy.org.au/grt

# Docker
docker_users:
  - "{{ galaxy_user.name }}"
docker_daemon_options:
  data-root: /mnt/docker-data

# Job conf limits (defined here to be available to job conf and tpv)
job_conf_limits:
  environments:
    interactive_pulsar:
      tags:
      - registered_user_concurrent_jobs_12
    pulsar-QLD:
      tags:
      - registered_user_concurrent_jobs_12
    pulsar-azure:
      tags:
      - registered_user_concurrent_jobs_12
    pulsar-azure-1-gpu:
      tags:
      - registered_user_concurrent_jobs_12
    pulsar-azure-gpu:
      tags:
      - registered_user_concurrent_jobs_12
    pulsar-high-mem1:
      tags:
      - registered_user_concurrent_jobs_12
    pulsar-high-mem2:
      tags:
      - registered_user_concurrent_jobs_12
    pulsar-mel2:
      tags:
      - registered_user_concurrent_jobs_12
    pulsar-mel3:
      tags:
      - registered_user_concurrent_jobs_12
    pulsar-nci-training:
      tags:
      - registered_user_concurrent_jobs_12
    pulsar-qld-blast:
      tags:
      - registered_user_concurrent_jobs_12
    pulsar-mel-blast:
      tags:
      - registered_user_concurrent_jobs_12
    pulsar-qld-high-mem0:
      tags:
      - registered_user_concurrent_jobs_12
    pulsar-qld-high-mem1:
      tags:
      - registered_user_concurrent_jobs_12
    pulsar-qld-high-mem2:
      tags:
      - registered_user_concurrent_jobs_12
    slurm:
      tags:
      - registered_user_concurrent_jobs_12
    slurm-training:
      tags:
      - registered_user_concurrent_jobs_12
    pulsar-qld-gpu1:
      tags:
      - registered_user_concurrent_jobs_12
    pulsar-qld-gpu2:
      tags:
      - registered_user_concurrent_jobs_12
    pulsar-qld-gpu3:
      tags:
      - registered_user_concurrent_jobs_12
    pulsar-qld-gpu4:
      tags:
      - registered_user_concurrent_jobs_12
    pulsar-qld-gpu5:
      tags:
      - registered_user_concurrent_jobs_12
  limits:
  - type: anonymous_user_concurrent_jobs
    value: 1
  - type: destination_user_concurrent_jobs
    tag: registered_user_concurrent_jobs_10
    value: 10
  - type: destination_user_concurrent_jobs
    tag: registered_user_concurrent_jobs_12
    value: 12
  - type: destination_user_concurrent_jobs
    tag: registered_user_concurrent_jobs_25
    value: 25

  - type: destination_total_concurrent_jobs
    id: slurm
    value: 80
  - type: destination_user_concurrent_jobs
    id: slurm
    value: 5

  - type: destination_total_concurrent_jobs
    id: slurm-training
    value: 100
  - type: destination_user_concurrent_jobs
    id: slurm-training
    value: 4

  - type: destination_total_concurrent_jobs
    id: pulsar-mel2
    value: 25

  - type: destination_total_concurrent_jobs
    id: pulsar-mel3
    value: 30

  - type: destination_user_concurrent_jobs
    id: pulsar-mel3
    value: 4  # Note: this limit is only for when pulsar-mel3 is being used as the training pulsar, there should be no user limit otherwise

  - type: destination_user_concurrent_jobs
    id: pulsar-nci-training
    value: 4

  - type: destination_total_concurrent_jobs
    id: pulsar-nci-training
    value: 40

  - type: destination_total_concurrent_jobs
    id: pulsar-high-mem1
    value: 6

  - type: destination_total_concurrent_jobs
    id: pulsar-high-mem2
    value: 6

  - type: destination_total_concurrent_jobs
    id: interactive_pulsar
    value: 10

  - type: destination_user_concurrent_jobs
    id: interactive_pulsar
    value: 2

  - type: destination_total_concurrent_jobs
    id: pulsar-qld-high-mem0
    value: 20

  - type: destination_total_concurrent_jobs
    id: pulsar-qld-high-mem1
    value: 20

  - type: destination_total_concurrent_jobs
    id: pulsar-qld-high-mem2
    value: 20

  - type: destination_total_concurrent_jobs
    id: pulsar-QLD
    value: 30

  - type: destination_total_concurrent_jobs
    id: pulsar-qld-blast
    value: 6
  - type: destination_user_concurrent_jobs
    id: pulsar-qld-blast
    value: 2

  - type: destination_total_concurrent_jobs
    id: pulsar-mel-blast
    value: 4
  - type: destination_user_concurrent_jobs
    id: pulsar-mel-blast
    value: 1

  - type: destination_total_concurrent_jobs
    id: pulsar-azure
    value: 4
  - type: destination_user_concurrent_jobs
    id: pulsar-azure
    value: 1

  - type: destination_total_concurrent_jobs
    id: pulsar-azure-gpu
    value: 8

  - type: destination_total_concurrent_jobs
    id: pulsar-qld-gpu1
    value: 6

  - type: destination_total_concurrent_jobs
    id: pulsar-qld-gpu2
    value: 6

  - type: destination_total_concurrent_jobs
    id: pulsar-qld-gpu3
    value: 6

  - type: destination_total_concurrent_jobs
    id: pulsar-qld-gpu4
    value: 6

  - type: destination_total_concurrent_jobs
    id: pulsar-qld-gpu5
    value: 6

# Singularity and docker volumes
slurm_singularity_volumes_list:
  - $job_directory:rw
  - $galaxy_root:ro
  - $tool_directory:ro
  - /mnt/user-data-volA:ro
  - /mnt/user-data-volB:ro
  - /mnt/user-data-volC:ro
  - /mnt/user-data-volD:ro
  - "{{ qld_file_mounts_path }}:ro"
  - /mnt/custom-indices:ro
  - /cvmfs/data.galaxyproject.org:ro
  - /tmp:rw

pulsar_singularity_volumes_list:
  - $job_directory:rw
  - $tool_directory:ro
  - /mnt/custom-indices:ro
  - /cvmfs/data.galaxyproject.org:ro
  - /tmp:rw

slurm_docker_volumes_list: "{{ slurm_singularity_volumes_list }}"
pulsar_docker_volumes_list: "{{ pulsar_singularity_volumes_list }}"

# comma separated strings for the job conf
slurm_singularity_volumes: "{{ slurm_singularity_volumes_list | join(',') }}"
pulsar_singularity_volumes: "{{ pulsar_singularity_volumes_list | join(',') }}"
slurm_docker_volumes: "{{ slurm_docker_volumes_list | join(',') }}"
pulsar_docker_volumes: "{{ pulsar_docker_volumes_list | join(',') }}"

singularity_default_container_id: "/cvmfs/singularity.galaxyproject.org/all/python:3.8.3"

# delete-tmp-jwds role for removing job working directories from /mnt/tmp
dt_remote_ip: "{{ hostvars['galaxy-job-nfs']['internal_ip'] }}"
dt_ssh_key: /home/ubuntu/.ssh/ubuntu_maintenance_key
dt_cron_hour: "20"
dt_cron_minute: "00"
dt_skip_cron_setup: true
dt_connection_string: "postgres://reader:{{ galaxy_db_reader_password }}@{{ hostvars['galaxy-db']['internal_ip'] }}:5432/galaxy" # TODO: generalise connection string

# NFS stuff (for exporting so galaxy-handlers can mount)
nfs_exports:
  - "{{ galaxy_root }}  {{ hostvars['galaxy'].internal_ip.split('.')[:-1] | join('.') }}.0/24(rw,async,no_root_squash,no_subtree_check)"

# Subdomains
galaxy_subsite_base_domain: "usegalaxy.org.au"
galaxy_subsite_default_welcome: "https://site.usegalaxy.org.au"
galaxy_subsite_dir: /mnt/galaxy/subsites
galaxy_subsites:
  - name: genome
    brand: Genome Lab
    iframe: "https://labs.usegalaxy.org.au/?content_root=https://github.com/galaxyproject/galaxy_codex/blob/main/communities/genome/lab/usegalaxy.org.au.yml"
    wallpaper: false # If wallpaper is true, then files/subsites/{name}.png will be copied to /static/dist/{name}.png, and can be used exactly like that in the CSS.
    tool_sections: []
    custom_css: |
      #masthead .navbar-nav>li.active {
        background: #2c3067 !important;
      }
      #masthead {
        background: #3a3e87 !important;
      }
      #masthead a.navbar-brand::after {
        content: "Australia - Genome Lab";
      }
      #masthead .navbar-text {
        display: none;
      }
  - name: proteomics
    brand: Proteomics Lab
    iframe: "https://labs.usegalaxy.org.au/?content_root=https://github.com/galaxyproject/galaxy_codex/blob/main/communities/proteomics/lab/usegalaxy.org.au.yml"
    wallpaper: false # If wallpaper is true, then files/subsites/{name}.png will be copied to /static/dist/{name}.png, and can be used exactly like that in the CSS.
    tool_sections:
      - getext
      - send
      - collection_operations
      - textutil
      - filter
      - group
      - fastafastq
      - fastq_quality_control
      - sambam
      - bed
      - vcfbcf
      - nanopore
      - convert
      - liftOver
      - bxops
      - mimodd
      - extract_features
      - fetchAlignSeq
      - assembly
      - annotation
      - mapping
      - variant_calling
      - chip_seq
      - rna_seq
      - multiple_alignments
      - bacterial_typing
      - hgv
      - phylogenetics
      - epigenetics
      - expression_tools
      - genome_editing
      - viral_tools
      - mothur
      - metagenomic_analysis
      - dna_metabarcoding
      - proteomics
      - proteomic_ai
      - metabolomics
      - chemicaltoolbox
      - picard
      - deeptools
      - emboss
      - blast
      - rseqc
      - gemini_tools
      - ivar
      - single_cell
      - rad_seq
      - hicexplorer
      - bb_tools
      - presto
      - qiime_2
      - stats
      - plots
      - machine_learning
      - imaging
      - climate_analysis
      - species_abundance
      - hca_single_cell
      - other_tools
    custom_css: |
      #masthead .navbar-nav>li.active {
        background: #402668 !important;
      }
      #masthead {
        background: #542a95 !important;
      }
      #masthead a.navbar-brand::after {
        content: "Australia - Proteomics Lab";
      }
      #masthead .navbar-text {
        display: none;
      }
  - name: singlecell
    brand: Single Cell Lab
    iframe: "https://labs.usegalaxy.org.au/?content_root=https://github.com/galaxyproject/galaxy_codex/blob/main/communities/spoc/lab/usegalaxy.org.au.yml"
    wallpaper: false
    tool_sections:
      - getext
      - send
      - collection_operations
      - textutil
      - filter
      - group
      - fastafastq
      - fastq_quality_control
      - sambam
      - bed
      - vcfbcf
      - nanopore
      - convert
      - liftOver
      - bxops
      - mimodd
      - extract_features
      - fetchAlignSeq
      - assembly
      - annotation
      - mapping
      - variant_calling
      - chip_seq
      - rna_seq
      - multiple_alignments
      - bacterial_typing
      - hgv
      - phylogenetics
      - epigenetics
      - expression_tools
      - genome_editing
      - viral_tools
      - mothur
      - metagenomic_analysis
      - dna_metabarcoding
      - proteomics
      - proteomic_ai
      - metabolomics
      - chemicaltoolbox
      - picard
      - deeptools
      - emboss
      - blast
      - rseqc
      - gemini_tools
      - ivar
      - single_cell
      - rad_seq
      - hicexplorer
      - bb_tools
      - presto
      - qiime_2
      - stats
      - plots
      - machine_learning
      - imaging
      - climate_analysis
      - species_abundance
      - hca_single_cell
      - other_tools
    custom_css: |
      #masthead .navbar-nav>li.active {
        background: #236c6f !important;
      }
      #masthead {
        background: #209693 !important;
      }
      #masthead a.navbar-brand::after {
        content: "Australia - Single Cell Lab";
      }
      #masthead .navbar-text {
        display: none;
      }
  - name: microbiology
    brand: Microbiology Lab
    iframe: "https://labs.usegalaxy.org.au/?content_root=https://github.com/galaxyproject/galaxy_codex/blob/main/communities/microgalaxy/lab/usegalaxy.org.au.yml"
    wallpaper: false
    tool_sections:
      - getext
      - send
      - collection_operations
      - textutil
      - filter
      - group
      - fastafastq
      - fastq_quality_control
      - sambam
      - bed
      - vcfbcf
      - nanopore
      - convert
      - liftOver
      - bxops
      - mimodd
      - extract_features
      - fetchAlignSeq
      - assembly
      - annotation
      - mapping
      - variant_calling
      - chip_seq
      - rna_seq
      - multiple_alignments
      - bacterial_typing
      - hgv
      - phylogenetics
      - epigenetics
      - expression_tools
      - genome_editing
      - viral_tools
      - mothur
      - metagenomic_analysis
      - dna_metabarcoding
      - proteomics
      - proteomic_ai
      - metabolomics
      - chemicaltoolbox
      - picard
      - deeptools
      - emboss
      - blast
      - rseqc
      - gemini_tools
      - ivar
      - single_cell
      - rad_seq
      - hicexplorer
      - bb_tools
      - presto
      - qiime_2
      - stats
      - plots
      - machine_learning
      - imaging
      - climate_analysis
      - species_abundance
      - hca_single_cell
      - other_tools
    custom_css: |
      #masthead .navbar-nav>li.active {
        background: #1b5b62 !important;
      }
      #masthead {
        background: #1a7b92 !important;
      }
      #masthead a.navbar-brand::after {
        content: "Australia - Microbiology Lab";
      }
      #masthead .navbar-text {
        display: none;
      }

webhook_plugins:
  - subdomain_switcher_24.2
  - toolmsg_24.2
  - tips
  # These ones aren't in the playbook as they are included in the galaxy repo:
  - demo
  - gtn
  - news

toolmsg_messages:
# - tool_id: to match subject.startsWith(tool_id)
#            Best to use remove version numbers and trailing slash
#            e.g. toolshed.g2.bx.psu.edu/repos/galaxyp/diann/diann
#   message: A custom HTML message to be displayed for this tool
#   class: bootstrap class [primary, info, success, warning, danger]

  - tool_id: toolshed.g2.bx.psu.edu/repos/galaxyp/diann/diann
    message: >
      Access rights to this tool have changed, please
      <a href="https://site.usegalaxy.org.au/request/access/diann"
         target="_blank"
      >
        apply for access
      </a>
      to continue using this tool.
    class: warning

  - tool_id: toolshed.g2.bx.psu.edu/repos/galaxy-australia/alphafold2/alphafold
    message: >
      This tool has been updated in line with best practice recommendations. <a href="https://site.usegalaxy.org.au/notice/32" target="_blank">Click for more information.</a>
    class: info

  - tool_id: toolshed.g2.bx.psu.edu/repos/iuc/ena_upload/ena_upload
    message: >
      Please note, this upload tool is suitable for small datasets only and may not perform well for
      <strong>datasets that exceed several GB</strong>.
      Please contact
      <a href="mailto:help@genome.edu.au">help@genome.edu.au</a>
      for support on how to move large datasets.
    class: info

# ssh config, only for ubuntu
ssh_config_id_file: "/home/{{ ssh_config_user }}/.ssh/internal_hop_key"
ssh_config_user: ubuntu
ssh_config_hosts: "{{ groups['galaxy_group'] }}"

walle_user_name: ubuntu
walle_user_group: ubuntu
walle_tool: interactive_tool
walle_virtualenv: "{{ galaxy_venv_dir }}"
walle_bashrc: "{{ galaxy_root }}/walle/.bashrc"
walle_malware_database_location: "{{ galaxy_root }}/walle"
walle_malware_database_force_update: true
walle_filesize_min: 0
walle_verbose: false
walle_kill: false
walle_slack_alerts: true
walle_slack_api_token: "{{ vault_galaxy_australia_slack_api_token }}"
walle_slack_channel_id: CJVFNFKGD
walle_cron_minute: "0"
walle_extra_env_vars:
  GALAXY_PULSAR_APP_CONF: "{{ galaxy_config_dir }}/pulsar_app.yml"
  PGHOST: "{{ db_address }}"
  GALAXY_CONFIG_FILE: "{{ galaxy_config_file }}"
