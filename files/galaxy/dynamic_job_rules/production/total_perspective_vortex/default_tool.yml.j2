global:
  default_inherits: default

tools:
  default:
    cores: 1
    mem: int(cores or 1) * 3.8
    env: 
      HDF5_USE_FILE_LOCKING: "FALSE"
      SINGULARITYENV_HDF5_USE_FILE_LOCKING: "FALSE"
      _JAVA_OPTIONS: -Xmx{int(mem or 3.8)}G -Xms1G -Djava.io.tmpdir=$_GALAXY_JOB_TMP_DIR
      SINGULARITYENV__JAVA_OPTIONS: -Xmx{int(mem or 3.8)}G -Xms1G -Djava.io.tmpdir=$_GALAXY_JOB_TMP_DIR
    context:
      partition: main
      test_cores: 1  # TODO: test_mem?
      max_concurrent_job_count_for_tool_total: null
      max_concurrent_job_count_for_tool_user: null
      require_login: false

      # pulsar score
      pulsar_score: null  # scores set in tool_pulsar_scores.yml. Not every tool will have a score
      pulsar_score_pulsar_threshold: 10  # above this score, prefer to send jobs to pulsar
      pulsar_score_slurm_threshold: 0.3  # below this score, prefer to send jobs to slurm
      pulsar_score_slurm_max_cores: 12  # if cores >= this number, do not set preference for slurm
      pulsar_score_slurm_max_mem: 48  # if mem >= this number, do not set preference for slurm
 
      # resolver selection
      minimum_singularity_version: null

      # singularity and docker volumes
      slurm_singularity_volumes: "{{ slurm_singularity_volumes }}"
      slurm_docker_volumes: "{{ slurm_docker_volumes }}"
      pulsar_singularity_volumes: "{{ pulsar_singularity_volumes }}"
      pulsar_docker_volumes: "{{ pulsar_docker_volumes }}"
    scheduling:
      reject:
        - offline
    rules:
    - id: login_required_rule
      if: require_login is True and user is None
      fail: 'Please log in to use {tool.id}'
    - id: minimum_singularity_version_positive_rule
      if: minimum_singularity_version is not None and helpers.tool_version_gte(tool, minimum_singularity_version)
      params:
        singularity_enabled: true
    - id: minimum_singularity_version_negative_rule
      if: minimum_singularity_version is not None and helpers.tool_version_lt(tool, minimum_singularity_version)
      params:
        singularity_enabled: false
    - id: max_concurrent_job_count_for_tool_rule
      if: |
        total_limit_exceeded = False
        user_limit_exceeded = False
        if max_concurrent_job_count_for_tool_total:
          total_limit_exceeded = helpers.concurrent_job_count_for_tool(app, tool) >= max_concurrent_job_count_for_tool_total
        if max_concurrent_job_count_for_tool_user and not total_limit_exceeded:
          user_limit_exceeded = helpers.concurrent_job_count_for_tool(app, tool, user) >= max_concurrent_job_count_for_tool_user
        total_limit_exceeded or user_limit_exceeded
      execute: |
        from galaxy.jobs.mapper import JobNotReadyException
        raise JobNotReadyException()
    - id: pulsar_score_prefer_pulsar_rule
      if: |
        result = pulsar_score is not None and pulsar_score_pulsar_threshold is not None and (
          helpers.tag_values_match(entity, ['pulsar'], [])  # tool is eligible to run on pulsar
          and pulsar_score > pulsar_score_pulsar_threshold  # tool scores highly for resources used * runtime per GB of input/output data
        )
        if result:
          log.info(f"++ ---tpv rank debug::: preferring pulsar for tool {tool.id} (pulsar score {pulsar_score})")
        result
      scheduling:
        prefer:
          - pulsar
    - id: pulsar_score_prefer_slurm_rule
      if: |
        result = (
          # all(v is not None for v in [pulsar_score, pulsar_score_slurm_threshold, pulsar_score_slurm_max_cores, pulsar_score_slurm_max_mem])  # null checks in bulk like this do not seem to work
          pulsar_score is not None
          and pulsar_score_slurm_threshold is not None
          and pulsar_score_slurm_max_cores is not None
          and pulsar_score_slurm_max_mem is not None
          and any(isinstance(entity.mem, ntype) for ntype in [int, float]) and (
            helpers.tag_values_match(entity, ['pulsar'], [])  # tool is eligible to run on pulsar
            and pulsar_score < pulsar_score_slurm_threshold  # tool has a low score for resources used * runtime per GB of input/output data
            and int(entity.cores or 1) < pulsar_score_slurm_max_cores  # job resource requirements are not so high as to make automatic slurm assignment difficult 
            and float(entity.mem or 3.8) < pulsar_score_slurm_max_mem
        ))
        if result:
          log.info(f"++ ---tpv rank debug::: preferring slurm for tool {tool.id} (pulsar score {pulsar_score})")
        result
      scheduling:
        prefer:
          - slurm
    rank: |
      if len(candidate_destinations) <= 1:
        log.info("++ ---tpv rank debug: 1 or fewer destinations: returning candidate_destinations")
        final_destinations = candidate_destinations
      else:
        log.info(f"++ ---tpv rank debug: ranking destinations for job with tool {tool.id}, cores: {cores}, mem: {mem}")
        import random
        from sqlalchemy import text
        from tpv.core.entities import TagType

        raw_sql_query = """
            select destination_id, state, count(id), sum(cores), sum(mem)
            from (
                select id,
                      CAST((REGEXP_MATCHES(encode(destination_params, 'escape'),'ntasks=(\d+)'))[1] as INTEGER)  as cores,
                      CAST((REGEXP_MATCHES(encode(destination_params, 'escape'),'mem=(\d+)'))[1] as INTEGER)  as mem,
                      state,
                      destination_id
                from job
                where state='queued' or state='running'
                order by destination_id
            ) as foo
            group by destination_id, state;
        """

        # clarify cores/mem values to avoid mypy warnings throughout rank function
        entity_cores = int(cores or 1)
        entity_mem = float(mem or 3.8)

        # Special case: A job requiring a lot of memory that may run on slurm
        # will probably run sooner on a high memory node.
        # If there is more than one candidate destination, remove slurm.
        high_memory_threshold = 62.5
        if entity_mem > high_memory_threshold:
          candidate_destinations = [d for d in candidate_destinations if d.id != 'slurm']

        try:
          results = app.model.context.execute(text(raw_sql_query))
          db_queue_info = {}
          for row in results:
              # log.info(f"++ ---tpv rank debug: row returned by db query: {str(row)}")
              destination_id, state, count_id, sum_cores, sum_mem = row
              if not destination_id in db_queue_info:
                  db_queue_info[destination_id] = {
                    'queued': {'sum_cores': 0, 'sum_mem': 0.0, 'job_count': 0},
                    'running': {'sum_cores': 0, 'sum_mem': 0.0, 'job_count': 0},
                  }
              db_queue_info[destination_id][state] = {'sum_cores': sum_cores, 'sum_mem': sum_mem, 'job_count': count_id}

          def score_preferred(dest, ent):
              """
              Computes a compatibility score between tag sets based only on a 'prefer' tag in one matching a positive tag in the other.
              :param ent:
              :return:
              """
              def a_prefers_b(a_tags, b_tags):
                  return any(
                    tag for tag in a_tags.filter(tag_type = TagType.PREFER)
                    if list(b_tags.filter(tag_type = [TagType.ACCEPT, TagType.PREFER, TagType.REQUIRE], tag_value = tag.value))
                  )
              dest_tags = dest.tpv_dest_tags
              ent_tags = ent.tpv_tags
              score = 1 if a_prefers_b(dest_tags, ent_tags) or a_prefers_b(ent_tags, dest_tags) else 0
              return score

          def destination_availability_score(destination):
              if not destination.context.get('destination_total_mem') or not destination.context.get('destination_total_cores'):
                raise Exception(f"++ ---tpv rank debug: At least one of destination_total_mem, destination_total_cores is unavailable")
              destination_ids = [destination.id] + destination.context.get('shared_resource_destination_ids', [])
              destination_cores_committed = sum(
                  db_queue_info.get(d_id, {}).get(state, {}).get('sum_cores', 0)
                  for d_id in destination_ids
                  for state in ['queued', 'running']
              )
              destination_mem_committed = sum(
                  db_queue_info.get(d_id, {}).get(state, {}).get('sum_mem', 0.0)
                  for d_id in destination_ids
                  for state in ['queued', 'running']
              )
              available_cores = destination.context['destination_total_cores'] - destination_cores_committed
              available_mem = float(destination.context['destination_total_mem']) - (float(destination_mem_committed)/1024)
 
              the_score = min(available_cores/entity_cores, available_mem/entity_mem) # returns the number of jobs with cores/mem that could run at destination
              log.info(f"++ ---tpv rank debug: The availablity score for destination {destination.id} is {the_score}")
              return the_score

          # New ranking method: consider memory use at destination as well as CPU use
          # Sort destinations by the number of jobs with these values of entity cores/mem that would be able to run there
          candidate_destinations.sort(key=lambda d: (-1 * score_preferred(d, entity), -1 * destination_availability_score(d), random.randint(0,9)))

          final_destinations = candidate_destinations
          log.info(f"++ ---tpv rank debug: destinations ranked: {[d.id for d in final_destinations]}")
        except Exception:
          log.exception("++ ---tpv rank debug: An error occurred with database query and/or surrounding logic. Using a weighted random candidate destination")
          final_destinations = helpers.weighted_random_sampling(candidate_destinations)
      final_destinations
