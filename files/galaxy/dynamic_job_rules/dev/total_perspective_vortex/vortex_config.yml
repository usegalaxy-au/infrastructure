global:
  default_inherits: default

tools:
  default:
    cores: 1
    mem: cores * 3 # note, some clusters will accept more than this
    env: {}
    params:
      nativeSpecification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --mem={mem*1024}"
    scheduling:
      prefer:
      accept:
        - general
      reject:
        - offline
    rules: []
    rank: |
      final_destinations = helpers.weighted_random_sampling(candidate_destinations)
      final_destinations
    #   import requests
    #   params = {
    #     'pretty': 'true',
    #     'db': 'pulsar-test',
    #     'q': 'SELECT last("percent_allocated") from "sinfo" group by "host"'
    #   }
    #   try:
    #     response = requests.get('http://stats.genome.edu.au:8086/query', params=params)
    #     data = response.json()
    #     cpu_by_destination = {s['tags']['host']:s['values'][0][1] for s in data.get('results')[0].get('series', [])}
    #     # sort by destination preference, and then by cpu usage
    #     candidate_destinations.sort(key=lambda d: (-1 * d.score(resource), cpu_by_destination.get(d.id)))
    #     final_destinations = candidate_destinations
    #   except Exception:
    #     log.exception("An error occurred while querying influxdb. Using a weighted random candidate destination")
    #     final_destinations = helpers.weighted_random_sampling(candidate_destinations)
    #   final_destinations
  pulsar_preferred:
    scheduling:
      prefer:
        - pulsar
  upload1:
    cores: 2
  '.*iuc/fasta_stats/fasta-stats.*':
    inherits: pulsar_preferred
    cores: 2
  '.*bwa_mem.*':
    inherits: pulsar_preferred
    cores: 2
  '.*hifiasm.*':
    inherits: pulsar_preferred
    cores: 2
    rules:
      - match: input_size >= 0.2
        cores: 16
users:
  default:
    rules:
      # - match: |
      #     from galaxy.jobs.rule_helper import RuleHelper  # this does not work and is probably taken care of by galaxy anyway

      #     retval = False
      #     if user:
      #       rule_helper = RuleHelper(app)
      #       job_limit = 10
      #       # job_limit = app.job_config.limits.registered_user_concurrent_jobs
      #       user_job_count = rule_helper.job_count(for_user_email=user.email)
      #       if user_job_count >= job_limit:
      #         retval = True
      #     retval
      #   fail: "User {user.id} has too many running jobs to schedule another"
      - match: |
          from galaxy.jobs.rule_helper import RuleHelper
          from vortex.core.entities import TagType
          if entity.tags.filter(tag_value='highmem'):
            rule_helper = RuleHelper(app)
            # Find all destinations that support highmem
            destinations = [d.id for d in mapper.destinations.values()
                            if any(d.tags.filter(tag_value='highmem',
                                   tag_type=[TagType.REQUIRE, TagType.PREFER, TagType.ACCEPT]))]
            count = rule_helper.job_count(
              for_user_email=user.email,
              for_destinations=destinations,
              for_job_states=['queued', 'running'],
            )
            if count > 4:
              retval = True
            else:
              retval = False
          else:
            retval = False
          retval
        fail: "You cannot have more than 4 high-mem jobs running concurrently"
  pulsar_user@usegalaxy.org.au:
    scheduling:
      require:
        - dev-pulsar
  pulsar_nci_test_user@usegalaxy.org.au:
    scheduling:
      require:
        - pulsar-nci-test

destinations:
  slurm:
    cores: 4
    mem: 15.5
    scheduling:
      accept:
        - pulsar
        - general
  pulsar_destination:
    cores: 4
    mem: 7.77
    scheduling:
      accept:
        - general
        - dev-pulsar
      require:
        - pulsar
  pulsar-nci-test:
    cores: 16
    mem: 48.19
    scheduling:
      accept:
        - general
        - pulsar-nci-test
        - offline
      require:
        - pulsar
