tools:
  pulsar_preferred:
    scheduling:
      prefer:
        - pulsar
  upload1:
    cores: 1
  
  toolshed.g2.bx.psu.edu/repos/bgruening/bionano_scaffold/bionano_scaffold/.*:
    cores: 2
    params:
      docker_enabled: true
      docker_volumes: '$defaults'
      docker_memory: '{mem}'
      docker_sudo: false
  toolshed.g2.bx.psu.edu/repos/iuc/fasta_stats/fasta-stats/.*:
    inherits: pulsar_preferred
    cores: 2
  toolshed.g2.bx.psu.edu/repos/iuc/maker/maker/.*:
    inherits: pulsar_preferred
    cores: 8
    mem: 30.39  
  #   env:
  #     MAKER_MPI: 1
  toolshed.g2.bx.psu.edu/repos/iuc/bbtools_callvariants/bbtools_callvariants/.*:
    inherits: pulsar_preferred
    cores: 16
  toolshed.g2.bx.psu.edu/repos/iuc/bbtools_bbmap/bbtools_bbmap/.*:
    inherits: pulsar_preferred
    cores: 16
  toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/*:
    inherits: pulsar_preferred
    rules:
      - match: |
          ### helper function
          from vortex.core.entities import Tool
          from galaxy import model
          import re

          def concurrent_jobs_for_tool(app, entity, user=None):
            if user and isinstance(entity, Tool):
              try:
                get_tool_id_regex = re.compile('Tool: (?P<tool_id_regex>[^,]*)')
                tool_id_regex = re.match(get_tool_id_regex, entity.id).groupdict()['tool_id_regex']
              except Exception:
                return 0
              query = app.model.context.query(model.Job)
              if user:
                query = query.filter(model.Job.table.c.user_id == user.id)
              query = query.filter(model.Job.table.c.state.in_(['queued', 'running']))
              query = query.filter(model.Job.table.c.tool_id.regexp_match(tool_id_regex))
              return query.count()
            return 0  # if entity is not tool, return 0
          ### end helper function

          concurrent_jobs_for_tool(app, entity, user) > 0 # limits fastqc to one job per user at a time
        execute: |
          from galaxy.jobs.mapper import JobNotReadyException; raise JobNotReadyException()
  toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa_mem/.*:
    cores: 1
    rules:
      # send all paired end jobs to pulsar to test argument matching
      - match: |
          helpers.job_args_match(job, app, {'fastq_input': {"fastq_input_selector": "paired"}})
        scheduling:
          prefer:
            - pulsar
  toolshed.g2.bx.psu.edu/repos/bgruening/hifiasm/hifiasm/.*:
    inherits: pulsar_preferred
    cores: 2
    rules:
      - match: input_size >= 0.2
        cores: 16
  toolshed.g2.bx.psu.edu/repos/lparsons/cutadapt/cutadapt/.*:
    cores: 2
    rules:
    - match: input_size >= 2
      cores: 3
  toolshed.g2.bx.psu.edu/repos/galaxyp/maxquant/maxquant/.*:
    cores: 2
    rules:
    - match: input_size >= 2
      cores: 3
  toolshed.g2.bx.psu.edu/repos/bgruening/flye/flye/.*:
    inherits: pulsar_preferred
    cores: 8
    mem: 30.39
  toolshed.g2.bx.psu.edu/repos/chemteam/gmx_sim/gmx_sim/.*:
    inherits: pulsar_preferred
    cores: 4
  toolshed.g2.bx.psu.edu/repos/chemteam/gmx_solvate/gmx_solvate/.*:
    inherits: pulsar_preferred
    cores: 4
  toolshed.g2.bx.psu.edu/repos/chemteam/gmx_setup/gmx_setup/.*:
    inherits: pulsar_preferred
    cores: 4
  toolshed.g2.bx.psu.edu/repos/chemteam/gmx_editconf/gmx_editconf/.*:
    inherits: pulsar_preferred
    cores: 4
  toolshed.g2.bx.psu.edu/repos/chemteam/gmx_em/gmx_em/.*:
    inherits: pulsar_preferred
    cores: 4
  alphafold:
    cores: 40
    mem: 107
    params:
      dependency_resolution: 'none'
      singularity_enabled: true
      singularity_run_extra_arguments: --nv
      singularity_volumes: "$job_directory:ro,$tool_directory:ro,$job_directory/outputs:rw,$working_directory:rw,/data/alphafold_databases:/data:ro"
    scheduling:
      accept:
        - pulsar
        - pulsar-eu-gpu-alpha
      require:
        - alphafold
  toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.9+galaxy1:
    cores: 2
    params:
      singularity_enabled: True
      singularity_volumes: '$defaults'
      singularity_default_container_id: '/cvmfs/singularity.galaxyproject.org/all/python:3.8.3'
  toolshed.g2.bx.psu.edu/repos/lparsons/htseq_count/htseq_count/.*:
    scheduling:
      require:
        - pulsar
        - pulsar-nci-test
    cores: 2
    params:
      singularity_enabled: True
      singularity_volumes: '$defaults'
      singularity_default_container_id: '/cvmfs/singularity.galaxyproject.org/all/python:3.8.3'

users:
  default:
    rules:
      # - match: |
      #     from galaxy.jobs.rule_helper import RuleHelper  # this is a test of raising JobNotReadyException, jobs per user limit is already taken care of by galaxy

      #     retval = False
      #     if user:
      #       rule_helper = RuleHelper(app)
      #       job_limit = 1
      #       # job_limit = app.job_config.limits.registered_user_concurrent_jobs
      #       user_job_count = rule_helper.job_count(for_user_email=user.email, for_job_states=['queued', 'running'])
      #       if user_job_count >= job_limit:
      #         retval = True
      #     retval
      #   execute: |
      #       from galaxy.jobs.mapper import JobNotReadyException
      #       raise JobNotReadyException()
      - match: |
          from galaxy.jobs.rule_helper import RuleHelper
          from vortex.core.entities import TagType
          if entity.tags.filter(tag_value='highmem'):
            rule_helper = RuleHelper(app)
            # Find all destinations that support highmem
            destinations = [d.id for d in mapper.destinations.values()
                            if any(d.tags.filter(tag_value='highmem',
                                   tag_type=[TagType.REQUIRE, TagType.PREFER, TagType.ACCEPT]))]
            count = rule_helper.job_count(
              for_user_email=user.email,
              for_destinations=destinations,
              for_job_states=['queued', 'running'],
            )
            if count > 4:
              retval = True
            else:
              retval = False
          else:
            retval = False
          retval
        execute: |
          from galaxy.jobs.mapper import JobNotReadyException; raise JobNotReadyException()
  pulsar_user@usegalaxy.org.au:
    rules:
      - match: tool.id.startswith('toolshed') or tool.id.startswith('testtoolshed')  # this locks out all local tools, might need to make exceptions for these by id
        scheduling:
          require:
            - pulsar  # if pulsar requires the pulsar tag, nothing without the pulsar tag will go there
            - dev-pulsar
  pulsar_nci_test_user@usegalaxy.org.au:
    rules:
      - match: tool.id.startswith('toolshed') or tool.id.startswith('testtoolshed')
        scheduling:
          require:
            - pulsar
            - pulsar-nci-test
  eugpu@usegalaxy.org.au:
    rules:
      - match: tool.id.startswith('toolshed') or tool.id.startswith('testtoolshed') or tool.id == 'alphafold'
        scheduling:
          require:
            - pulsar
            - pulsar-eu-gpu-alpha
  jenkins_bot@usegalaxy.org.au:
    cores: 1
    scheduling:
      require:
        - slurm
  pulsar_azure_0@genome.edu.au:
    rules:
      - match: tool.id.startswith('toolshed') or tool.id.startswith('testtoolshed')
        scheduling:
          require:
            - pulsar
            - pulsar-azure-0
      - match: tool.id == 'alphafold'
        scheduling:
          accept:
            - pulsar
          require:
            - pulsar-azure-0
        cores: 6
        mem: 105
        params:
          nativeSpecification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --mem={round(mem*1024)} --partition=azuregpu1"
          submit_native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --mem={round(mem*1024)} --partition=azuregpu1"


destinations:
  slurm:
    cores: 4
    mem: 7.59
    scheduling:
      accept:
        - pulsar
        - slurm
  pulsar_destination:
    cores: 4
    mem: 7.77
    scheduling:
      accept:
        - dev-pulsar
      require:
        - pulsar
  pulsar-nci-test:
    cores: 16
    mem: 48.19
    scheduling:
      accept:
        - general
        - pulsar-nci-test
      require:
        - pulsar
  pulsar-eu-gpu-test:
    cores: 40
    mem: 107
    scheduling:
      accept:
        - general
      require:
        - pulsar
        - pulsar-eu-gpu-test
  pulsar-eu-gpu-alpha:
    cores: 40
    mem: 107
    scheduling:
      accept:
        - general
        - alphafold
      require:
        - pulsar
        - pulsar-eu-gpu-alpha
  pulsar-azure-0-std:  # TODO: this needs higher cores/mem for alphafold
    cores: 6
    mem: 107
    scheduling:
      accept:
        - general
        - alphafold
      require:
        - pulsar
        - pulsar-azure-0
