tools:
  upload1:
    cores: 1
  toolshed.g2.bx.psu.edu/repos/bgruening/bionano_scaffold/bionano_scaffold/.*:
    scheduling:
      accept:
        - pulsar
    params:
      docker_enabled: true
      docker_memory: '{mem}G'
    cores: 8
  toolshed.g2.bx.psu.edu/repos/iuc/links/links/.*:
    params:
      singularity_enabled: true
      dependency_resolution: 'none'
    cores: 2
  toolshed.g2.bx.psu.edu/repos/iuc/fasta_stats/fasta-stats/.*:
    scheduling:
      accept:
        - pulsar
    cores: 2
  toolshed.g2.bx.psu.edu/repos/iuc/maker/maker/.*:
    scheduling:
      accept:
        - pulsar
    cores: 8
    mem: 30.39
  #   env:
  #     MAKER_MPI: 1
  toolshed.g2.bx.psu.edu/repos/iuc/bbtools_callvariants/bbtools_callvariants/.*:
    scheduling:
      accept:
        - pulsar
    cores: 8
  toolshed.g2.bx.psu.edu/repos/iuc/bbtools_bbmap/bbtools_bbmap/.*:
    scheduling:
      accept:
        - pulsar
    cores: 16
  toolshed.g2.bx.psu.edu/repos/devteam/fastqc/fastqc/*:
    scheduling:
      accept:
        - pulsar
    rules:
      - if: |
          ### helper function
          from vortex.core.entities import Tool
          from galaxy import model
          import re

          def concurrent_jobs_for_tool(app, entity, user=None):
            if user and isinstance(entity, Tool):
              try:
                get_tool_id_regex = re.compile('Tool: (?P<tool_id_regex>[^,]*)')
                tool_id_regex = re.match(get_tool_id_regex, entity.id).groupdict()['tool_id_regex']
              except Exception:
                return 0
              query = app.model.context.query(model.Job)
              if user:
                query = query.filter(model.Job.table.c.user_id == user.id)
              query = query.filter(model.Job.table.c.state.in_(['queued', 'running']))
              query = query.filter(model.Job.table.c.tool_id.regexp_match(tool_id_regex))
              return query.count()
            return 0  # if entity is not tool, return 0
          ### end helper function

          concurrent_jobs_for_tool(app, entity, user) > 0 # limits fastqc to one job per user at a time
        execute: |
          from galaxy.jobs.mapper import JobNotReadyException; raise JobNotReadyException()
  toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa_mem/.*:
    cores: 1
    scheduling:
      accept:
        - pulsar  
    rules:
      - if: user.email == "pulsar_azure_0@genome.edu.au" and input_size < 10
        scheduling:
          require:
            - pulsar-azure-0
        cores: 4
        mem: 15
        params:
          nativeSpecification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --mem={round(mem*1024)} --partition=small"
          submit_native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --mem={round(mem*1024)} --partition=small"
      - if: user.email == "pulsar_azure_0@genome.edu.au" and input_size >= 10
        scheduling:
          require:
            - pulsar-azure-0
        cores: 16
        mem: 60.8
        params:
          nativeSpecification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --mem={round(mem*1024)} --partition=medium"
          submit_native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --mem={round(mem*1024)} --partition=medium"


  toolshed.g2.bx.psu.edu/repos/bgruening/hifiasm/hifiasm/.*:
    scheduling:
      accept:
        - pulsar
    cores: 2
    rules:
      - if: input_size >= 0.2
        cores: 16
  toolshed.g2.bx.psu.edu/repos/lparsons/cutadapt/cutadapt/.*:
    cores: 2
    rules:
    - if: input_size >= 2
      cores: 3
  toolshed.g2.bx.psu.edu/repos/galaxyp/maxquant/maxquant/.*:
    cores: 2
    rules:
    - if: input_size >= 2
      cores: 3
  toolshed.g2.bx.psu.edu/repos/bgruening/flye/flye/.*:
    cores: 8
    mem: 30.39
    scheduling:
      accept:
        - pulsar
    rules:
      - if: user.email == "pulsar_azure_0@genome.edu.au"
        cores: 120
        mem: 1900
        params:
          nativeSpecification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --mem={round(mem*1024)} --partition=special"
          submit_native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --mem={round(mem*1024)} --partition=special"

  toolshed.g2.bx.psu.edu/repos/chemteam/gmx_sim/gmx_sim/.*:
    scheduling:
      accept:
        - pulsar
    cores: 4
  toolshed.g2.bx.psu.edu/repos/chemteam/gmx_solvate/gmx_solvate/.*:
    scheduling:
      accept:
        - pulsar
    cores: 4
  toolshed.g2.bx.psu.edu/repos/chemteam/gmx_setup/gmx_setup/.*:
    scheduling:
      accept:
        - pulsar
    cores: 4
  toolshed.g2.bx.psu.edu/repos/chemteam/gmx_editconf/gmx_editconf/.*:
    scheduling:
      accept:
        - pulsar
    cores: 4
  toolshed.g2.bx.psu.edu/repos/chemteam/gmx_em/gmx_em/.*:
    scheduling:
      accept:
        - pulsar
    cores: 4
  .*alphafold.*:  # match local or toolshed alphafold
    cores: 40
    mem: 107
    scheduling:
      accept:
        - pulsar
        - pulsar-eu-gpu-alpha
      require:
        - alphafold
    rules:
      - if: user.email == "pulsar_azure_0@genome.edu.au"
        scheduling:
          require:
            - pulsar-azure-0
        cores: 6
        mem: 106
        params:
          nativeSpecification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --mem={round(mem*1024)} --partition=azuregpu1"
          submit_native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --mem={round(mem*1024)} --partition=azuregpu1"
          docker_enabled: true
          docker_volumes: '$job_directory:ro,$tool_directory:ro,$job_directory/outputs:rw,$working_directory:rw,/data/alphafold_databases:/data:ro'
          docker_memory: 107G
          docker_sudo: false
          require_container: true
          docker_run_extra_arguments: "--gpus all --env ALPHAFOLD_AA_LENGTH_MIN=30 --env ALPHAFOLD_AA_LENGTH_MAX=2000"
          docker_set_user: '1000'
  toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.9+galaxy1:
    cores: 2
    params:
      singularity_enabled: True
      singularity_volumes: '$defaults'
      singularity_default_container_id: '/cvmfs/singularity.galaxyproject.org/all/python:3.8.3'
  toolshed.g2.bx.psu.edu/repos/iuc/trinity/trinity/.*:
    scheduling:
      accept:
        - pulsar
    rules:
      - if: user.email == "pulsar_azure_0@genome.edu.au" and input_size < 0.1
        scheduling:
          require:
            - pulsar-azure-0
        cores: 4
        mem: 15
        params:
          nativeSpecification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --mem={round(mem*1024)} --partition=small"
          submit_native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --mem={round(mem*1024)} --partition=small"
      - if: user.email == "pulsar_azure_0@genome.edu.au" and input_size >= 0.1
        scheduling:
          require:
            - pulsar-azure-0
        cores: 32
        mem: 121.6
        params:
          nativeSpecification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --mem={round(mem*1024)} --partition=large"
          submit_native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --mem={round(mem*1024)} --partition=large"
  
  toolshed.g2.bx.psu.edu/repos/lparsons/htseq_count/htseq_count/.*:
    scheduling:
      require:
        - pulsar
        - pulsar-nci-test
    cores: 2
    params:
      singularity_enabled: True
      singularity_volumes: '$defaults'
      singularity_default_container_id: '/cvmfs/singularity.galaxyproject.org/all/python:3.8.3'
  toolshed.g2.bx.psu.edu/repos/galaxy-australia/cactus_cactus/cactus_cactus/.*:
    cores: 16
    mem: 32
    params:
      singularity_enabled: True
      singularity_volumes: '$defaults,/cvmfs:ro'
      singularity_default_container_id: '/cvmfs/singularity.galaxyproject.org/all/python:3.8.3'
  toolshed.g2.bx.psu.edu/repos/galaxy-australia/cactus_export/cactus_export/.*:
    cores: 1
    params:
      singularity_enabled: True
      singularity_volumes: '$defaults,/cvmfs:ro'
      singularity_default_container_id: '/cvmfs/singularity.galaxyproject.org/all/python:3.8.3'
  pbgcpp:
    cores: 2
    params:
      singularity_enabled: True
      singularity_volumes: '$defaults,/cvmfs:ro'
      singularity_default_container_id: '/cvmfs/singularity.galaxyproject.org/all/python:3.8.3'
  pbmm2:
    cores: 2
    params:
      singularity_enabled: True
      singularity_volumes: '$defaults,/cvmfs:ro'
      singularity_default_container_id: '/cvmfs/singularity.galaxyproject.org/all/python:3.8.3'
  toolshed.g2.bx.psu.edu/repos/iracooke/protk_proteogenomics/.*:
    cores: 1
    params:
      singularity_enabled: True
      singularity_run_extra_arguments: '--writable-tmpfs'
      dependency_resolution: 'none'
  toolshed.g2.bx.psu.edu/repos/iracooke/xtandem/.*:
    cores: 1
    params:
      singularity_enabled: True
      singularity_run_extra_arguments: '--writable-tmpfs'
      dependency_resolution: 'none'
  toolshed.g2.bx.psu.edu/repos/iracooke/protk-galaxytools/.*:
    cores: 1
    params:
      singularity_enabled: True
      singularity_run_extra_arguments: '--writable-tmpfs'
      dependency_resolution: 'none'
  toolshed.g2.bx.psu.edu/repos/iracooke/tpp_prophets/.*:
    cores: 1
    params:
      singularity_enabled: True
      singularity_run_extra_arguments: '--writable-tmpfs'
      dependency_resolution: 'none'
  maxquant_test:
    scheduling:
       require:
         - pulsar
         - dev-pulsar
    cores: 2
    mem: 7.76
  toolshed.g2.bx.psu.edu/repos/bgruening/chemfp/.*:
    params:
      singularity_enabled: True

users:
  default:
    rules:
      # - if: |
      #     from galaxy.jobs.rule_helper import RuleHelper  # this is a test of raising JobNotReadyException, jobs per user limit is already taken care of by galaxy

      #     retval = False
      #     if user:
      #       rule_helper = RuleHelper(app)
      #       job_limit = 1
      #       # job_limit = app.job_config.limits.registered_user_concurrent_jobs
      #       user_job_count = rule_helper.job_count(for_user_email=user.email, for_job_states=['queued', 'running'])
      #       if user_job_count >= job_limit:
      #         retval = True
      #     retval
      #   execute: |
      #       from galaxy.jobs.mapper import JobNotReadyException
      #       raise JobNotReadyException()
      - if: |
          from galaxy.jobs.rule_helper import RuleHelper
          from vortex.core.entities import TagType
          if entity.tags.filter(tag_value='highmem'):
            rule_helper = RuleHelper(app)
            # Find all destinations that support highmem
            destinations = [d.id for d in mapper.destinations.values()
                            if any(d.tags.filter(tag_value='highmem',
                                   tag_type=[TagType.REQUIRE, TagType.PREFER, TagType.ACCEPT]))]
            count = rule_helper.job_count(
              for_user_email=user.email,
              for_destinations=destinations,
              for_job_states=['queued', 'running'],
            )
            if count > 4:
              retval = True
            else:
              retval = False
          else:
            retval = False
          retval
        execute: |
          from galaxy.jobs.mapper import JobNotReadyException; raise JobNotReadyException()
  pulsar_user@usegalaxy.org.au:
    rules:
      - if: tool.id.startswith('toolshed') or tool.id.startswith('testtoolshed')  # this locks out all local tools, might need to make exceptions for these by id
        scheduling:
          require:
            - pulsar  # if pulsar requires the pulsar tag, nothing without the pulsar tag will go there
            - dev-pulsar
  pulsar_nci_test_user@usegalaxy.org.au:
    rules:
      - if: tool.id.startswith('toolshed') or tool.id.startswith('testtoolshed')
        scheduling:
          require:
            - pulsar
            - pulsar-nci-test
  eugpu@usegalaxy.org.au:
    rules:
      - if: tool.id.startswith('toolshed') or tool.id.startswith('testtoolshed') or tool.id == 'alphafold'
        scheduling:
          require:
            - pulsar
            - pulsar-eu-gpu-alpha
  jenkins_bot@usegalaxy.org.au:
    cores: 1
    scheduling:
      require:
        - slurm
  pulsar_azure_0@genome.edu.au:
    rules:
      - if: tool.id.startswith('toolshed') or tool.id.startswith('testtoolshed')
        scheduling:
          require:
            - pulsar
            - pulsar-azure-0
  pulsar_azure_docker@genome.edu.au:
    rules:
      - if: tool.id.startswith('toolshed') or tool.id.startswith('testtoolshed')
        scheduling:
          require:
            - pulsar
            - pulsar-azure-docker
      - if: tool.id == 'alphafold'
        scheduling:
          accept:
            - pulsar
          require:
            - pulsar-azure-docker
        cores: 6
        mem: 105
        params:
          nativeSpecification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --mem={round(mem*1024)} --partition=azuregpu1"
          submit_native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --mem={round(mem*1024)} --partition=azuregpu1"

destinations:
  slurm:
    cores: 16
    mem: 62.5
    scheduling:
      accept:
        - slurm
  pulsar_destination:
    cores: 2
    mem: 7.76
    scheduling:
      accept:
        - dev-pulsar
      require:
        - pulsar
  pulsar-nci-test:
    cores: 16
    mem: 48.19
    scheduling:
      accept:
        - general
        - pulsar-nci-test
        - offline
      require:
        - pulsar
  pulsar-eu-gpu-test:
    cores: 40
    mem: 107
    scheduling:
      accept:
        - general
      require:
        - pulsar
        - pulsar-eu-gpu-test
  pulsar-eu-gpu-alpha:
    cores: 40
    mem: 107
    scheduling:
      accept:
        - general
        - alphafold
      require:
        - pulsar
        - pulsar-eu-gpu-alpha
  pulsar-azure-0-std:  # TODO: this needs higher cores/mem for alphafold
    cores: 120
    mem: 2000
    scheduling:
      accept:
        - general
        - alphafold
        - offline
      require:
        - pulsar
        - pulsar-azure-0
  pulsar-azure-0-docker:  # TODO: this needs higher cores/mem for alphafold
    cores: 6
    mem: 107
    scheduling:
      accept:
        - general
        - alphafold
        - offline
      require:
        - pulsar
        - pulsar-azure-docker
