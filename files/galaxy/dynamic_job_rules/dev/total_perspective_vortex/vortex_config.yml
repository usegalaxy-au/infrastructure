global:
  default_inherits: default

tools:
  default:
    cores: 1
    mem: cores * 3 # note, some clusters will accept more than this
    env: {}
    params:
      nativeSpecification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --mem={mem*1024}"
      submit_native_specification: "--nodes=1 --ntasks={cores} --ntasks-per-node={cores} --mem={mem*1024}"
    scheduling:
      prefer:
      accept:
        - general
      reject:
        - offline
    rules: []
    rank: |
      final_destinations = helpers.weighted_random_sampling(candidate_destinations)
      final_destinations
    #   import requests
    #   params = {
    #     'pretty': 'true',
    #     'db': 'pulsar-test',
    #     'q': 'SELECT last("percent_allocated") from "sinfo" group by "host"'
    #   }
    #   try:
    #     response = requests.get('http://stats.genome.edu.au:8086/query', params=params)
    #     data = response.json()
    #     cpu_by_destination = {s['tags']['host']:s['values'][0][1] for s in data.get('results')[0].get('series', [])}
    #     # sort by destination preference, and then by cpu usage
    #     candidate_destinations.sort(key=lambda d: (-1 * d.score(resource), cpu_by_destination.get(d.id)))
    #     final_destinations = candidate_destinations
    #   except Exception:
    #     log.exception("An error occurred while querying influxdb. Using a weighted random candidate destination")
    #     final_destinations = helpers.weighted_random_sampling(candidate_destinations)
    #   final_destinations
  pulsar_preferred:
    scheduling:
      prefer:
        - pulsar
  upload1:
    cores: 2
  
  toolshed.g2.bx.psu.edu/repos/bgruening/bionano_scaffold/bionano_scaffold/.*:
    cores: 2
    params:
      docker_enabled: true
      docker_volumes: '$defaults'
      docker_memory: '{mem}'
      docker_sudo: false
  toolshed.g2.bx.psu.edu/repos/iuc/fasta_stats/fasta-stats/.*:
    inherits: pulsar_preferred
    cores: 20
  toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa_mem/.*:
    cores: 1
    rules:
      # send all paired end jobs to pulsar just because
      - match: |
          from functools import reduce

          arguments = {'fastq_input': {"fastq_input_selector": "paired"}}

          def get_keys_from_dict(dl, keys_list):

            # This function builds a list using the keys from nest dictionaries (and has been copied from galaxy dynamic_tool_destination.py along with most of this block)

            if isinstance(dl, dict):
                keys_list.extend(dl.keys())
                for x in dl.values():
                    get_keys_from_dict(x, keys_list)
            elif isinstance(dl, list):
                for x in dl:
                    get_keys_from_dict(x, keys_list)

          options = job.get_param_values(app)
          matched = True
          # check if the args in the config file are available
          for arg in arguments:
              arg_dict = {arg: arguments[arg]}
              arg_keys_list = []
              get_keys_from_dict(arg_dict, arg_keys_list)
              try:
                  options_value = reduce(dict.__getitem__, arg_keys_list, options)
                  arg_value = reduce(dict.__getitem__, arg_keys_list, arg_dict)
                  if (arg_value != options_value):
                      matched = False
              except KeyError:
                  matched = False 
          matched         
        scheduling:
          prefer:
            - pulsar
  toolshed.g2.bx.psu.edu/repos/bgruening/hifiasm/hifiasm/.*:
    inherits: pulsar_preferred
    cores: 2
    rules:
      - match: input_size >= 0.2
        cores: 16
users:
  default:
    rules:
      # - match: |
      #     from galaxy.jobs.rule_helper import RuleHelper  # this is a test of raising JobNotReadyException, jobs per user limit is already taken care of by galaxy

      #     retval = False
      #     if user:
      #       rule_helper = RuleHelper(app)
      #       job_limit = 1
      #       # job_limit = app.job_config.limits.registered_user_concurrent_jobs
      #       user_job_count = rule_helper.job_count(for_user_email=user.email, for_job_states=['queued', 'running'])
      #       if user_job_count >= job_limit:
      #         retval = True
      #     retval
      #   execute: |
      #       from galaxy.jobs.mapper import JobNotReadyException
      #       raise JobNotReadyException()
      - match: |
          from galaxy.jobs.rule_helper import RuleHelper
          from vortex.core.entities import TagType
          if entity.tags.filter(tag_value='highmem'):
            rule_helper = RuleHelper(app)
            # Find all destinations that support highmem
            destinations = [d.id for d in mapper.destinations.values()
                            if any(d.tags.filter(tag_value='highmem',
                                   tag_type=[TagType.REQUIRE, TagType.PREFER, TagType.ACCEPT]))]
            count = rule_helper.job_count(
              for_user_email=user.email,
              for_destinations=destinations,
              for_job_states=['queued', 'running'],
            )
            if count > 4:
              retval = True
            else:
              retval = False
          else:
            retval = False
          retval
        fail: "You cannot have more than 4 high-mem jobs running concurrently"
  pulsar_user@usegalaxy.org.au:
    rules:
      - match: tool.id.startswith('toolshed') or tool.id.startswith('testtoolshed')  # this locks out all local tools, might need to make exceptions for these by id
        scheduling:
          require:
            - pulsar  # if pulsar requires the pulsar tag, nothing without the pulsar tag will go there
            - dev-pulsar
  pulsar_nci_test_user@usegalaxy.org.au:
    rules:
      - match: tool.id.startswith('toolshed') or tool.id.startswith('testtoolshed')
        scheduling:
          require:
            - pulsar
            - pulsar-nci-test
  eugpu@usegalaxy.org.au:
    rules:
      - match: tool.id.startswith('toolshed') or tool.id.startswith('testtoolshed')
        scheduling:
          require:
            - pulsar
            - pulsar-eu-gpu-test
  jenkins_bot@usegalaxy.org.au:
    cores: 1
    scheduling:
      require:
        - slurm

destinations:
  slurm:
    cores: 4
    mem: 15.5
    scheduling:
      accept:
        - pulsar
        - general
        - slurm
  pulsar_destination:
    cores: 4
    mem: 7.77
    scheduling:
      accept:
        - general
        - dev-pulsar
      require:
        - pulsar
  pulsar-nci-test:
    cores: 16
    mem: 48.19
    scheduling:
      accept:
        - general
        - pulsar-nci-test
      require:
        - pulsar
  pulsar-eu-gpu-test:
    cores: 40
    mem: 110000
    scheduling:
      accept:
        - general
        - pulsar-eu-gpu-test
      require:
        - pulsar
